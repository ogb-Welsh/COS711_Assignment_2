{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "import yaml\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"Configuration\"\"\"\n",
    "\n",
    "with open(\"config.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# random_seed = random.randint(0, 100000)\n",
    "random_seed = 123\n",
    "\n",
    "print_graphs = config[\"print_graphs\"]\n",
    "perform_grid_search = config[\"perform_grid_search\"]\n",
    "features = config[\"features\"]\n",
    "target = config[\"target\"]\n",
    "num_epochs = config[\"num_epochs\"]\n",
    "learning_rate = config[\"learning_rate\"]\n",
    "dropout_rate = config[\"dropout_rate\"]\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Classes\"\"\"\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_rate):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.ln1 = nn.LayerNorm(256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.ln2 = nn.LayerNorm(128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.ln3 = nn.LayerNorm(64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.ln4 = nn.LayerNorm(32)\n",
    "        self.output_layer = nn.Linear(32, output_size)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.ln1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.ln2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.ln3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.ln4(self.fc4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Functions\"\"\"\n",
    "\n",
    "\n",
    "def cap_outliers_percentiles(df, feature, lower_percentile=5, upper_percentile=95):\n",
    "    lower_limit = np.percentile(df[feature], lower_percentile)\n",
    "    upper_limit = np.percentile(df[feature], upper_percentile)\n",
    "\n",
    "    df[feature] = np.where(df[feature] < lower_limit, lower_limit, df[feature])\n",
    "    df[feature] = np.where(df[feature] > upper_limit, upper_limit, df[feature])\n",
    "\n",
    "\n",
    "def plot_feature_distributions(data, data_imputed, features):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, feature in enumerate(features):\n",
    "        plt.subplot(4, 3, i + 1)\n",
    "        sns.histplot(\n",
    "            data[feature].dropna(),\n",
    "            kde=True,\n",
    "            label=\"Before Imputation\",\n",
    "            color=\"blue\",\n",
    "            bins=30,\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        sns.histplot(\n",
    "            data_imputed[feature].dropna(),\n",
    "            kde=True,\n",
    "            label=\"After Imputation\",\n",
    "            color=\"red\",\n",
    "            bins=30,\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        plt.title(f\"{feature}\")\n",
    "        plt.xlabel(\"\")\n",
    "        plt.ylabel(\"\")\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_box_plots_comparison(data, data_imputed, features):\n",
    "    plt.figure(figsize=(15, 12))\n",
    "\n",
    "    for i, feature in enumerate(features):\n",
    "        plt.subplot(4, 3, i + 1)\n",
    "\n",
    "        df_before_plot = data[[feature]].copy()\n",
    "        df_before_plot[\"Imputation Status\"] = \"Before Imputation\"\n",
    "\n",
    "        df_after_plot = data_imputed[[feature]].copy()\n",
    "        df_after_plot[\"Imputation Status\"] = \"After Imputation\"\n",
    "\n",
    "        df_plot = pd.concat([df_before_plot, df_after_plot], ignore_index=True)\n",
    "\n",
    "        sns.boxplot(\n",
    "            x=\"Imputation Status\",\n",
    "            y=feature,\n",
    "            data=df_plot,\n",
    "            hue=\"Imputation Status\",\n",
    "            palette=\"Set2\",\n",
    "            showfliers=True,\n",
    "        )\n",
    "\n",
    "        plt.title(f\"{feature}\")\n",
    "        plt.xlabel(\"\")\n",
    "        plt.ylabel(\"\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_correlation_heatmaps(data, data_imputed, features):\n",
    "    corr_before = data[features].corr()\n",
    "    corr_after = data_imputed[features].corr()\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(corr_before, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    plt.title(\"Correlation Before Imputation\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.heatmap(corr_after, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    plt.title(\"Correlation After Imputation\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def single_plot_feature_distributions(data, features):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, feature in enumerate(features):\n",
    "        plt.subplot(4, 3, i + 1)\n",
    "        sns.histplot(\n",
    "            data[feature].dropna(),\n",
    "            kde=True,\n",
    "            label=\"Normalized\",\n",
    "            color=\"blue\",\n",
    "            bins=30,\n",
    "            alpha=1,\n",
    "        )\n",
    "        plt.title(f\"{feature}\")\n",
    "        plt.xlabel(\"\")\n",
    "        plt.ylabel(\"\")\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def trainer(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    early_stopping,\n",
    "    x_train_tensor,\n",
    "    y_train_tensor,\n",
    "    x_val_tensor,\n",
    "    y_val_tensor,\n",
    "    num_epochs,\n",
    "):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        outputs = model(x_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss = loss.item()\n",
    "\n",
    "        val_accuracy, val_loss = evaluate_model(\n",
    "            model, x_val_tensor, y_val_tensor, criterion\n",
    "        )\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch + 1}/{num_epochs}], Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\"\n",
    "            )\n",
    "\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping triggered, Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "\n",
    "            return train_loss, val_loss, val_accuracy, (epoch + 1)\n",
    "\n",
    "\n",
    "def evaluate_model(model, x_tensor, y_tensor, criterion):\n",
    "    model.eval()\n",
    "    total_samples = len(y_tensor)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(x_tensor)\n",
    "        loss = criterion(outputs, y_tensor).item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        accuracy = (predicted == y_tensor).sum().item() / total_samples\n",
    "\n",
    "    return accuracy, loss\n",
    "\n",
    "\n",
    "def grid_search(\n",
    "    learning_rates,\n",
    "    dropout_rates,\n",
    "    input_size,\n",
    "    output_size,\n",
    "    x,\n",
    "    y_encoded,\n",
    "    num_epochs,\n",
    "    k_folds,\n",
    "):\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=123)\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        for dr in dropout_rates:\n",
    "\n",
    "            fold_train_losses = []\n",
    "            fold_val_losses = []\n",
    "            fold_val_accuracies = []\n",
    "\n",
    "            for fold, (train_idx, val_idx) in enumerate(kf.split(x)):\n",
    "                print(\n",
    "                    f\"Learning Rate {lr}, Dropout Rate {dr}, Fold {fold + 1}/{k_folds}\"\n",
    "                )\n",
    "\n",
    "                x_train, x_val = x[train_idx], x[val_idx]\n",
    "                y_train, y_val = y_encoded[train_idx], y_encoded[val_idx]\n",
    "\n",
    "                x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "                y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "                x_val_tensor = torch.tensor(x_val, dtype=torch.float32)\n",
    "                y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "                model = MLP(input_size, output_size, dr)\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                early_stopping = EarlyStopping(patience=50, delta=0.001)\n",
    "\n",
    "                final_train_loss, final_val_loss, final_val_accuracy, total_epochs = (\n",
    "                    trainer(\n",
    "                        model,\n",
    "                        criterion,\n",
    "                        optimizer,\n",
    "                        early_stopping,\n",
    "                        x_train_tensor,\n",
    "                        y_train_tensor,\n",
    "                        x_val_tensor,\n",
    "                        y_val_tensor,\n",
    "                        num_epochs,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                fold_train_losses.append(final_train_loss)\n",
    "                fold_val_losses.append(final_val_loss)\n",
    "                fold_val_accuracies.append(final_val_accuracy)\n",
    "\n",
    "            avg_train_loss = np.mean(fold_train_losses)\n",
    "            avg_val_loss = np.mean(fold_val_losses)\n",
    "            avg_val_accuracy = np.mean(fold_val_accuracies)\n",
    "            std_train_loss = np.std(fold_val_losses)\n",
    "            std_val_loss = np.std(fold_val_losses)\n",
    "            std_val_accuracy = np.std(fold_val_losses)\n",
    "\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"learning_rate\": lr,\n",
    "                    \"dropout_rate\": dr,\n",
    "                    \"avg_train_loss\": avg_train_loss,\n",
    "                    \"avg_val_loss\": avg_val_loss,\n",
    "                    \"avg_val_accuracy\": avg_val_accuracy,\n",
    "                    \"std_train_loss\": std_train_loss,\n",
    "                    \"std_val_loss\": std_val_loss,\n",
    "                    \"std_val_accuracy\": std_val_accuracy,\n",
    "                    \"total_epochs\": total_epochs,\n",
    "                }\n",
    "            )\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Read Data\"\"\"\n",
    "\n",
    "data = pd.read_csv(\"almond_data.csv\")\n",
    "\n",
    "data = data.iloc[:, 1:]\n",
    "\n",
    "data.head()\n",
    "\n",
    "data.columns = data.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Impute Features\"\"\"\n",
    "\n",
    "impute_features = [\n",
    "    \"Length (major axis)\",\n",
    "    \"Width (minor axis)\",\n",
    "    \"Thickness (depth)\",\n",
    "    \"Area\",\n",
    "    \"Perimeter\",\n",
    "    \"Solidity\",\n",
    "    \"Compactness\",\n",
    "    \"Extent\",\n",
    "    \"Convex hull(convex area)\",\n",
    "]\n",
    "\n",
    "knn_imputer = KNNImputer(n_neighbors=53)  # n_neighbors = root of dataset size\n",
    "\n",
    "data_imputed = data.copy()\n",
    "\n",
    "types = data[target].unique()\n",
    "for almond_type in types:\n",
    "\n",
    "    type_data = data[data[target] == almond_type].copy()\n",
    "\n",
    "    type_features = type_data[impute_features]\n",
    "\n",
    "    imputed_values = knn_imputer.fit_transform(type_features)\n",
    "\n",
    "    type_data[impute_features] = imputed_values\n",
    "\n",
    "    data_imputed.update(type_data)\n",
    "\n",
    "data_imputed[\"Roundness\"] = (4 * data_imputed[\"Area\"]) / (\n",
    "    np.pi * data_imputed[\"Length (major axis)\"] ** 2\n",
    ")\n",
    "\n",
    "data_imputed[\"Aspect Ratio\"] = (\n",
    "    data_imputed[\"Length (major axis)\"] / data_imputed[\"Width (minor axis)\"]\n",
    ")\n",
    "\n",
    "data_imputed[\"Eccentricity\"] = np.sqrt(\n",
    "    1 - (data_imputed[\"Width (minor axis)\"] / data_imputed[\"Length (major axis)\"]) ** 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cap Outliers\"\"\"\n",
    "\n",
    "for feature in features:\n",
    "    cap_outliers_percentiles(data_imputed, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_graphs:\n",
    "    plot_feature_distributions(data, data_imputed, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_graphs:\n",
    "    plot_box_plots_comparison(data, data_imputed, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_graphs:\n",
    "    plot_correlation_heatmaps(data, data_imputed, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Normalize Data\"\"\"\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "normalized_data = data_imputed.copy()\n",
    "normalized_data[features] = scaler.fit_transform(data_imputed[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_graphs:\n",
    "    single_plot_feature_distributions(normalized_data, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data setup\"\"\"\n",
    "\n",
    "x = normalized_data[features].values\n",
    "y = normalized_data[target].values\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "input_size = len(features)\n",
    "output_size = len(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Grid search with k fold cross validation using seed 123\"\"\"\n",
    "\n",
    "if perform_grid_search:\n",
    "\n",
    "    learning_rates = [0.001, 0.01, 0.1]\n",
    "    dropout_rates = [0.0, 0.2, 0.5]\n",
    "\n",
    "    os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"COS711_Assignment_2.ipynb\"\n",
    "    wandb.init(project=\"COS 711_Assignment 2\")\n",
    "\n",
    "    set_seed(123)\n",
    "    grid_search(\n",
    "        learning_rates,\n",
    "        dropout_rates,\n",
    "        input_size,\n",
    "        output_size,\n",
    "        x,\n",
    "        y_encoded,\n",
    "        num_epochs,\n",
    "        k_folds=5,\n",
    "    )\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_tensor size: torch.Size([2018, 12])\n",
      "y_train_tensor size: torch.Size([2018])\n",
      "x_val_tensor size: torch.Size([505, 12])\n",
      "y_val_tensor size: torch.Size([505])\n",
      "x_test_tensor size: torch.Size([280, 12])\n",
      "y_test_tensor size: torch.Size([280])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Train, validation and test set splitting\"\"\"\n",
    "\n",
    "set_seed(random_seed)\n",
    "\n",
    "x_test, x_temp, y_test, y_temp = train_test_split(\n",
    "    x, y_encoded, train_size=0.1, random_state=1\n",
    ")\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_temp, y_temp, train_size=0.8, random_state=random_seed\n",
    ")\n",
    "\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "x_val_tensor = torch.tensor(x_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "print(\"x_train_tensor size:\", x_train_tensor.size())\n",
    "print(\"y_train_tensor size:\", y_train_tensor.size())\n",
    "print(\"x_val_tensor size:\", x_val_tensor.size())\n",
    "print(\"y_val_tensor size:\", y_val_tensor.size())\n",
    "print(\"x_test_tensor size:\", x_test_tensor.size())\n",
    "print(\"y_test_tensor size:\", y_test_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/10000], Training Loss: 0.3480, Validation Loss: 0.2546, Validation Accuracy: 0.9069\n",
      "Epoch [200/10000], Training Loss: 0.2162, Validation Loss: 0.1559, Validation Accuracy: 0.9505\n",
      "Epoch [300/10000], Training Loss: 0.1594, Validation Loss: 0.1426, Validation Accuracy: 0.9465\n",
      "Epoch [400/10000], Training Loss: 0.1261, Validation Loss: 0.1121, Validation Accuracy: 0.9644\n",
      "Early stopping triggered, Epoch [472/10000]\n",
      "Epoch [472/10000], Training Loss: 0.1006, Validation Loss: 0.1063, Validation Accuracy: 0.9663\n"
     ]
    }
   ],
   "source": [
    "model = MLP(input_size, output_size, dropout_rate)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=50, delta=0.001)\n",
    "\n",
    "final_train_loss, final_val_loss, final_val_accuracy, total_epochs = trainer(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    early_stopping,\n",
    "    x_train_tensor,\n",
    "    y_train_tensor,\n",
    "    x_val_tensor,\n",
    "    y_val_tensor,\n",
    "    num_epochs,\n",
    ")\n",
    "\n",
    "print(f\"Epoch [{total_epochs}/{num_epochs}], Training Loss: {final_train_loss:.4f}, Validation Loss: {final_val_loss:.4f}, Validation Accuracy: {final_val_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
