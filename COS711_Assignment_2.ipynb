{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "import yaml\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"Configuration\"\"\"\n",
    "\n",
    "with open(\"config.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# random_seed = random.randint(0, 100000)\n",
    "random_seed = 123\n",
    "\n",
    "print_graphs = config[\"print_graphs\"]\n",
    "features = config[\"features\"]\n",
    "target = config[\"target\"]\n",
    "num_epochs = config[\"num_epochs\"]\n",
    "learning_rate = config[\"learning_rate\"]\n",
    "dropout_rate = config[\"dropout_rate\"]\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Classes\"\"\"\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_rate):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.ln1 = nn.LayerNorm(256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.ln2 = nn.LayerNorm(128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.ln3 = nn.LayerNorm(64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.ln4 = nn.LayerNorm(32)\n",
    "        self.output_layer = nn.Linear(32, output_size)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.ln1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.ln2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.ln3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.ln4(self.fc4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Functions\"\"\"\n",
    "\n",
    "\n",
    "def cap_outliers_percentiles(df, feature, lower_percentile=5, upper_percentile=95):\n",
    "    lower_limit = np.percentile(df[feature], lower_percentile)\n",
    "    upper_limit = np.percentile(df[feature], upper_percentile)\n",
    "\n",
    "    df[feature] = np.where(df[feature] < lower_limit, lower_limit, df[feature])\n",
    "    df[feature] = np.where(df[feature] > upper_limit, upper_limit, df[feature])\n",
    "\n",
    "\n",
    "def plot_feature_distributions(data, data_imputed, features):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, feature in enumerate(features):\n",
    "        plt.subplot(4, 3, i + 1)\n",
    "        sns.histplot(\n",
    "            data[feature].dropna(),\n",
    "            kde=True,\n",
    "            label=\"Before Imputation\",\n",
    "            color=\"blue\",\n",
    "            bins=30,\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        sns.histplot(\n",
    "            data_imputed[feature].dropna(),\n",
    "            kde=True,\n",
    "            label=\"After Imputation\",\n",
    "            color=\"red\",\n",
    "            bins=30,\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        plt.title(f\"{feature}\")\n",
    "        plt.xlabel(\"\")\n",
    "        plt.ylabel(\"\")\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_box_plots_comparison(data, data_imputed, features):\n",
    "    plt.figure(figsize=(15, 12))\n",
    "\n",
    "    for i, feature in enumerate(features):\n",
    "        plt.subplot(4, 3, i + 1)\n",
    "\n",
    "        df_before_plot = data[[feature]].copy()\n",
    "        df_before_plot[\"Imputation Status\"] = \"Before Imputation\"\n",
    "\n",
    "        df_after_plot = data_imputed[[feature]].copy()\n",
    "        df_after_plot[\"Imputation Status\"] = \"After Imputation\"\n",
    "\n",
    "        df_plot = pd.concat([df_before_plot, df_after_plot], ignore_index=True)\n",
    "\n",
    "        sns.boxplot(\n",
    "            x=\"Imputation Status\",\n",
    "            y=feature,\n",
    "            data=df_plot,\n",
    "            hue=\"Imputation Status\",\n",
    "            palette=\"Set2\",\n",
    "            showfliers=True,\n",
    "        )\n",
    "\n",
    "        plt.title(f\"{feature}\")\n",
    "        plt.xlabel(\"\")\n",
    "        plt.ylabel(\"\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_correlation_heatmaps(data, data_imputed, features):\n",
    "    corr_before = data[features].corr()\n",
    "    corr_after = data_imputed[features].corr()\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(corr_before, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    plt.title(\"Correlation Before Imputation\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.heatmap(corr_after, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    plt.title(\"Correlation After Imputation\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def single_plot_feature_distributions(data, features):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, feature in enumerate(features):\n",
    "        plt.subplot(4, 3, i + 1)\n",
    "        sns.histplot(\n",
    "            data[feature].dropna(),\n",
    "            kde=True,\n",
    "            label=\"Normalized\",\n",
    "            color=\"blue\",\n",
    "            bins=30,\n",
    "            alpha=1,\n",
    "        )\n",
    "        plt.title(f\"{feature}\")\n",
    "        plt.xlabel(\"\")\n",
    "        plt.ylabel(\"\")\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def trainer(\n",
    "    model, criterion, optimizer, early_stopping, x_train_tensor, y_train_tensor, x_val_tensor, y_val_tensor, num_epochs\n",
    "):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        outputs = model(x_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss = loss.item()\n",
    "\n",
    "        val_accuracy, val_loss = evaluate_model(model, x_val_tensor, y_val_tensor, criterion)\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch + 1}/{num_epochs}], Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\"\n",
    "            )\n",
    "\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping triggered, Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "\n",
    "            return train_loss, val_loss, val_accuracy, (epoch+1)\n",
    "\n",
    "\n",
    "def evaluate_model(model, x_tensor, y_tensor, criterion):\n",
    "    model.eval()\n",
    "    total_samples = len(y_tensor)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(x_tensor)\n",
    "        loss = criterion(outputs, y_tensor).item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        accuracy = (predicted == y_tensor).sum().item() / total_samples\n",
    "\n",
    "    return accuracy, loss\n",
    "\n",
    "\n",
    "def grid_search(learning_rates, dropout_rates, input_size, output_size, x, y_encoded, num_epochs, k_folds=5):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    \n",
    "    for lr in learning_rates:\n",
    "        for dr in dropout_rates:\n",
    "                \n",
    "                fold_train_losses = []\n",
    "                fold_val_losses = []\n",
    "                fold_val_accuracies = []\n",
    "                \n",
    "                for fold, (train_idx, val_idx) in enumerate(kf.split(x)):\n",
    "                    print(f\"Learning Rate {lr}, Dropout Rate {dr}, Fold {fold + 1}/{k_folds}\")\n",
    "\n",
    "                    x_train, x_val = x[train_idx], x[val_idx]\n",
    "                    y_train, y_val = y_encoded[train_idx], y_encoded[val_idx]\n",
    "\n",
    "                    x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "                    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "                    x_val_tensor = torch.tensor(x_val, dtype=torch.float32)\n",
    "                    y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "                    model = MLP(input_size, output_size, dr)\n",
    "                    criterion = nn.CrossEntropyLoss()\n",
    "                    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                    early_stopping = EarlyStopping(patience=50, delta=0.001)\n",
    "\n",
    "                    final_train_loss, final_val_loss, final_val_accuracy, total_epochs = trainer(\n",
    "                        model, criterion, optimizer, early_stopping,\n",
    "                        x_train_tensor, y_train_tensor, x_val_tensor, y_val_tensor, num_epochs\n",
    "                    )\n",
    "\n",
    "                    fold_train_losses.append(final_train_loss)\n",
    "                    fold_val_losses.append(final_val_loss)\n",
    "                    fold_val_accuracies.append(final_val_accuracy)\n",
    "\n",
    "                # Calculate the average results across all folds\n",
    "                avg_train_loss = np.mean(fold_train_losses)\n",
    "                avg_val_loss = np.mean(fold_val_losses)\n",
    "                avg_val_accuracy = np.mean(fold_val_accuracies)\n",
    "                std_train_loss = np.std(fold_val_losses)\n",
    "                std_val_loss = np.std(fold_val_losses)\n",
    "                std_val_accuracy = np.std(fold_val_losses)\n",
    "\n",
    "                # Log the results to wandb\n",
    "                wandb.log({\n",
    "                    'learning_rate': lr,\n",
    "                    'dropout_rate': dr,\n",
    "                    'avg_train_loss': avg_train_loss,\n",
    "                    'avg_val_loss': avg_val_loss,\n",
    "                    'avg_val_accuracy': avg_val_accuracy,\n",
    "                    'std_train_loss': std_train_loss,\n",
    "                    'std_val_loss': std_val_loss,\n",
    "                    'std_val_accuracy': std_val_accuracy,\n",
    "                    'total_epochs': total_epochs\n",
    "                })\n",
    "                \n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Read Data\"\"\"\n",
    "\n",
    "data = pd.read_csv(\"almond_data.csv\")\n",
    "\n",
    "data = data.iloc[:, 1:]\n",
    "\n",
    "data.head()\n",
    "\n",
    "data.columns = data.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Impute Features\"\"\"\n",
    "\n",
    "impute_features = [\n",
    "    \"Length (major axis)\",\n",
    "    \"Width (minor axis)\",\n",
    "    \"Thickness (depth)\",\n",
    "    \"Area\",\n",
    "    \"Perimeter\",\n",
    "    \"Solidity\",\n",
    "    \"Compactness\",\n",
    "    \"Extent\",\n",
    "    \"Convex hull(convex area)\",\n",
    "]\n",
    "\n",
    "knn_imputer = KNNImputer(n_neighbors=53)  # n_neighbors = root of dataset size\n",
    "\n",
    "data_imputed = data.copy()\n",
    "\n",
    "types = data[target].unique()\n",
    "for almond_type in types:\n",
    "\n",
    "    type_data = data[data[target] == almond_type].copy()\n",
    "\n",
    "    type_features = type_data[impute_features]\n",
    "\n",
    "    imputed_values = knn_imputer.fit_transform(type_features)\n",
    "\n",
    "    type_data[impute_features] = imputed_values\n",
    "\n",
    "    data_imputed.update(type_data)\n",
    "\n",
    "data_imputed[\"Roundness\"] = (4 * data_imputed[\"Area\"]) / (\n",
    "    np.pi * data_imputed[\"Length (major axis)\"] ** 2\n",
    ")\n",
    "\n",
    "data_imputed[\"Aspect Ratio\"] = (\n",
    "    data_imputed[\"Length (major axis)\"] / data_imputed[\"Width (minor axis)\"]\n",
    ")\n",
    "\n",
    "data_imputed[\"Eccentricity\"] = np.sqrt(\n",
    "    1 - (data_imputed[\"Width (minor axis)\"] / data_imputed[\"Length (major axis)\"]) ** 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cap Outliers\"\"\"\n",
    "\n",
    "for feature in features:\n",
    "    cap_outliers_percentiles(data_imputed, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_graphs:\n",
    "    plot_feature_distributions(data, data_imputed, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_graphs:\n",
    "    plot_box_plots_comparison(data, data_imputed, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_graphs:\n",
    "    plot_correlation_heatmaps(data, data_imputed, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Normalize Data\"\"\"\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "normalized_data = data_imputed.copy()\n",
    "normalized_data[features] = scaler.fit_transform(data_imputed[features])\n",
    "\n",
    "# print(normalized_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_graphs:\n",
    "    single_plot_feature_distributions(normalized_data, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''Data setup'''\n",
    "\n",
    "x = normalized_data[features].values\n",
    "y = normalized_data[target].values\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "input_size = len(features)\n",
    "output_size = len(label_encoder.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mu21432962\u001b[0m (\u001b[33mu21432962-university-of-pretoria\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/oliver_welsh/Oliver_Welsh/Code/GitHub/COS711_Assignment_2/wandb/run-20240921_023908-o7nmtpuy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/u21432962-university-of-pretoria/COS%20711_Assignment%202/runs/o7nmtpuy' target=\"_blank\">misty-dream-8</a></strong> to <a href='https://wandb.ai/u21432962-university-of-pretoria/COS%20711_Assignment%202' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/u21432962-university-of-pretoria/COS%20711_Assignment%202' target=\"_blank\">https://wandb.ai/u21432962-university-of-pretoria/COS%20711_Assignment%202</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/u21432962-university-of-pretoria/COS%20711_Assignment%202/runs/o7nmtpuy' target=\"_blank\">https://wandb.ai/u21432962-university-of-pretoria/COS%20711_Assignment%202/runs/o7nmtpuy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.001, Dropout Rate 0.0, Fold 1/5\n",
      "Epoch [100/10000], Training Loss: 0.1260, Validation Loss: 0.1936, Validation Accuracy: 0.9323\n",
      "Epoch [200/10000], Training Loss: 0.1365, Validation Loss: 0.2036, Validation Accuracy: 0.9216\n",
      "Early stopping triggered, Epoch [205/10000]\n",
      "Learning Rate 0.001, Dropout Rate 0.0, Fold 2/5\n",
      "Epoch [100/10000], Training Loss: 0.0491, Validation Loss: 0.1522, Validation Accuracy: 0.9554\n",
      "Early stopping triggered, Epoch [164/10000]\n",
      "Learning Rate 0.001, Dropout Rate 0.0, Fold 3/5\n",
      "Epoch [100/10000], Training Loss: 0.0548, Validation Loss: 0.1995, Validation Accuracy: 0.9394\n",
      "Early stopping triggered, Epoch [196/10000]\n",
      "Learning Rate 0.001, Dropout Rate 0.0, Fold 4/5\n",
      "Epoch [100/10000], Training Loss: 0.0566, Validation Loss: 0.1431, Validation Accuracy: 0.9464\n",
      "Epoch [200/10000], Training Loss: 0.0130, Validation Loss: 0.1270, Validation Accuracy: 0.9554\n",
      "Early stopping triggered, Epoch [237/10000]\n",
      "Learning Rate 0.001, Dropout Rate 0.0, Fold 5/5\n",
      "Epoch [100/10000], Training Loss: 0.0759, Validation Loss: 0.1264, Validation Accuracy: 0.9607\n",
      "Early stopping triggered, Epoch [177/10000]\n",
      "Learning Rate 0.001, Dropout Rate 0.2, Fold 1/5\n",
      "Epoch [100/10000], Training Loss: 0.3125, Validation Loss: 0.2800, Validation Accuracy: 0.8966\n",
      "Epoch [200/10000], Training Loss: 0.2082, Validation Loss: 0.1791, Validation Accuracy: 0.9340\n",
      "Epoch [300/10000], Training Loss: 0.1481, Validation Loss: 0.1662, Validation Accuracy: 0.9483\n",
      "Epoch [400/10000], Training Loss: 0.1199, Validation Loss: 0.1319, Validation Accuracy: 0.9590\n",
      "Early stopping triggered, Epoch [449/10000]\n",
      "Learning Rate 0.001, Dropout Rate 0.2, Fold 2/5\n",
      "Epoch [100/10000], Training Loss: 0.3318, Validation Loss: 0.2140, Validation Accuracy: 0.9287\n",
      "Epoch [200/10000], Training Loss: 0.2058, Validation Loss: 0.1353, Validation Accuracy: 0.9537\n",
      "Epoch [300/10000], Training Loss: 0.1647, Validation Loss: 0.1260, Validation Accuracy: 0.9590\n",
      "Epoch [400/10000], Training Loss: 0.1313, Validation Loss: 0.0993, Validation Accuracy: 0.9643\n",
      "Early stopping triggered, Epoch [448/10000]\n",
      "Learning Rate 0.001, Dropout Rate 0.2, Fold 3/5\n",
      "Epoch [100/10000], Training Loss: 0.3031, Validation Loss: 0.2847, Validation Accuracy: 0.8966\n",
      "Epoch [200/10000], Training Loss: 0.2000, Validation Loss: 0.2429, Validation Accuracy: 0.9091\n",
      "Epoch [300/10000], Training Loss: 0.1345, Validation Loss: 0.2187, Validation Accuracy: 0.9376\n",
      "Early stopping triggered, Epoch [370/10000]\n",
      "Learning Rate 0.001, Dropout Rate 0.2, Fold 4/5\n",
      "Epoch [100/10000], Training Loss: 0.3388, Validation Loss: 0.2393, Validation Accuracy: 0.9179\n",
      "Epoch [200/10000], Training Loss: 0.1839, Validation Loss: 0.1657, Validation Accuracy: 0.9339\n",
      "Epoch [300/10000], Training Loss: 0.1427, Validation Loss: 0.1335, Validation Accuracy: 0.9446\n",
      "Epoch [400/10000], Training Loss: 0.1025, Validation Loss: 0.1108, Validation Accuracy: 0.9500\n",
      "Epoch [500/10000], Training Loss: 0.1067, Validation Loss: 0.1211, Validation Accuracy: 0.9518\n",
      "Early stopping triggered, Epoch [505/10000]\n",
      "Learning Rate 0.001, Dropout Rate 0.2, Fold 5/5\n",
      "Epoch [100/10000], Training Loss: 0.3276, Validation Loss: 0.2509, Validation Accuracy: 0.9036\n",
      "Epoch [200/10000], Training Loss: 0.1877, Validation Loss: 0.1578, Validation Accuracy: 0.9429\n",
      "Epoch [300/10000], Training Loss: 0.1478, Validation Loss: 0.1348, Validation Accuracy: 0.9518\n",
      "Epoch [400/10000], Training Loss: 0.1200, Validation Loss: 0.1103, Validation Accuracy: 0.9607\n",
      "Epoch [500/10000], Training Loss: 0.0930, Validation Loss: 0.1031, Validation Accuracy: 0.9750\n",
      "Epoch [600/10000], Training Loss: 0.0892, Validation Loss: 0.0904, Validation Accuracy: 0.9714\n",
      "Early stopping triggered, Epoch [654/10000]\n",
      "Learning Rate 0.001, Dropout Rate 0.5, Fold 1/5\n",
      "Epoch [100/10000], Training Loss: 0.6188, Validation Loss: 0.5174, Validation Accuracy: 0.8004\n",
      "Epoch [200/10000], Training Loss: 0.4857, Validation Loss: 0.3663, Validation Accuracy: 0.8592\n",
      "Epoch [300/10000], Training Loss: 0.3836, Validation Loss: 0.2865, Validation Accuracy: 0.9002\n",
      "Epoch [400/10000], Training Loss: 0.3352, Validation Loss: 0.2394, Validation Accuracy: 0.9144\n",
      "Epoch [500/10000], Training Loss: 0.2917, Validation Loss: 0.2105, Validation Accuracy: 0.9216\n",
      "Early stopping triggered, Epoch [557/10000]\n",
      "Learning Rate 0.001, Dropout Rate 0.5, Fold 2/5\n",
      "Epoch [100/10000], Training Loss: 0.6340, Validation Loss: 0.4965, Validation Accuracy: 0.8111\n",
      "Epoch [200/10000], Training Loss: 0.4893, Validation Loss: 0.3210, Validation Accuracy: 0.8913\n",
      "Epoch [300/10000], Training Loss: 0.3939, Validation Loss: 0.2239, Validation Accuracy: 0.9216\n",
      "Epoch [400/10000], Training Loss: 0.3340, Validation Loss: 0.1796, Validation Accuracy: 0.9447\n",
      "Epoch [500/10000], Training Loss: 0.2731, Validation Loss: 0.1526, Validation Accuracy: 0.9465\n",
      "Epoch [600/10000], Training Loss: 0.2424, Validation Loss: 0.1463, Validation Accuracy: 0.9501\n",
      "Early stopping triggered, Epoch [645/10000]\n",
      "Learning Rate 0.001, Dropout Rate 0.5, Fold 3/5\n",
      "Epoch [100/10000], Training Loss: 0.6348, Validation Loss: 0.4946, Validation Accuracy: 0.8021\n",
      "Epoch [200/10000], Training Loss: 0.4608, Validation Loss: 0.3451, Validation Accuracy: 0.8859\n",
      "Epoch [300/10000], Training Loss: 0.3688, Validation Loss: 0.2824, Validation Accuracy: 0.8895\n",
      "Epoch [400/10000], Training Loss: 0.3094, Validation Loss: 0.2716, Validation Accuracy: 0.8984\n",
      "Epoch [500/10000], Training Loss: 0.2747, Validation Loss: 0.2538, Validation Accuracy: 0.9055\n",
      "Epoch [600/10000], Training Loss: 0.2491, Validation Loss: 0.2498, Validation Accuracy: 0.9055\n",
      "Early stopping triggered, Epoch [627/10000]\n",
      "Learning Rate 0.001, Dropout Rate 0.5, Fold 4/5\n",
      "Epoch [100/10000], Training Loss: 0.6477, Validation Loss: 0.5119, Validation Accuracy: 0.7893\n",
      "Epoch [200/10000], Training Loss: 0.4903, Validation Loss: 0.3471, Validation Accuracy: 0.8696\n",
      "Epoch [300/10000], Training Loss: 0.3848, Validation Loss: 0.2496, Validation Accuracy: 0.8964\n",
      "Epoch [400/10000], Training Loss: 0.3302, Validation Loss: 0.1892, Validation Accuracy: 0.9250\n",
      "Epoch [500/10000], Training Loss: 0.2723, Validation Loss: 0.1576, Validation Accuracy: 0.9393\n",
      "Epoch [600/10000], Training Loss: 0.2683, Validation Loss: 0.1369, Validation Accuracy: 0.9482\n",
      "Early stopping triggered, Epoch [648/10000]\n",
      "Learning Rate 0.001, Dropout Rate 0.5, Fold 5/5\n",
      "Epoch [100/10000], Training Loss: 0.6266, Validation Loss: 0.4660, Validation Accuracy: 0.8125\n",
      "Epoch [200/10000], Training Loss: 0.4853, Validation Loss: 0.3173, Validation Accuracy: 0.8786\n",
      "Epoch [300/10000], Training Loss: 0.3662, Validation Loss: 0.2371, Validation Accuracy: 0.9143\n",
      "Epoch [400/10000], Training Loss: 0.2933, Validation Loss: 0.1877, Validation Accuracy: 0.9321\n",
      "Epoch [500/10000], Training Loss: 0.2590, Validation Loss: 0.1647, Validation Accuracy: 0.9339\n",
      "Epoch [600/10000], Training Loss: 0.2517, Validation Loss: 0.1490, Validation Accuracy: 0.9411\n",
      "Epoch [700/10000], Training Loss: 0.2197, Validation Loss: 0.1463, Validation Accuracy: 0.9393\n",
      "Early stopping triggered, Epoch [725/10000]\n",
      "Learning Rate 0.01, Dropout Rate 0.0, Fold 1/5\n",
      "Epoch [100/10000], Training Loss: 0.0670, Validation Loss: 0.1828, Validation Accuracy: 0.9358\n",
      "Early stopping triggered, Epoch [144/10000]\n",
      "Learning Rate 0.01, Dropout Rate 0.0, Fold 2/5\n",
      "Epoch [100/10000], Training Loss: 0.0851, Validation Loss: 0.1635, Validation Accuracy: 0.9465\n",
      "Early stopping triggered, Epoch [194/10000]\n",
      "Learning Rate 0.01, Dropout Rate 0.0, Fold 3/5\n",
      "Epoch [100/10000], Training Loss: 0.3446, Validation Loss: 0.4736, Validation Accuracy: 0.8610\n",
      "Early stopping triggered, Epoch [105/10000]\n",
      "Learning Rate 0.01, Dropout Rate 0.0, Fold 4/5\n",
      "Epoch [100/10000], Training Loss: 0.0495, Validation Loss: 0.1623, Validation Accuracy: 0.9464\n",
      "Early stopping triggered, Epoch [141/10000]\n",
      "Learning Rate 0.01, Dropout Rate 0.0, Fold 5/5\n",
      "Epoch [100/10000], Training Loss: 0.1354, Validation Loss: 0.1284, Validation Accuracy: 0.9518\n",
      "Early stopping triggered, Epoch [169/10000]\n",
      "Learning Rate 0.01, Dropout Rate 0.2, Fold 1/5\n",
      "Epoch [100/10000], Training Loss: 0.1549, Validation Loss: 0.1886, Validation Accuracy: 0.9198\n",
      "Epoch [200/10000], Training Loss: 0.0982, Validation Loss: 0.1332, Validation Accuracy: 0.9590\n",
      "Early stopping triggered, Epoch [214/10000]\n",
      "Learning Rate 0.01, Dropout Rate 0.2, Fold 2/5\n",
      "Epoch [100/10000], Training Loss: 0.1577, Validation Loss: 0.1466, Validation Accuracy: 0.9519\n",
      "Early stopping triggered, Epoch [189/10000]\n",
      "Learning Rate 0.01, Dropout Rate 0.2, Fold 3/5\n",
      "Epoch [100/10000], Training Loss: 0.1403, Validation Loss: 0.2308, Validation Accuracy: 0.9162\n",
      "Early stopping triggered, Epoch [149/10000]\n",
      "Learning Rate 0.01, Dropout Rate 0.2, Fold 4/5\n",
      "Epoch [100/10000], Training Loss: 0.1543, Validation Loss: 0.1495, Validation Accuracy: 0.9411\n",
      "Epoch [200/10000], Training Loss: 0.0854, Validation Loss: 0.1162, Validation Accuracy: 0.9500\n",
      "Epoch [300/10000], Training Loss: 0.0748, Validation Loss: 0.1341, Validation Accuracy: 0.9536\n",
      "Early stopping triggered, Epoch [346/10000]\n",
      "Learning Rate 0.01, Dropout Rate 0.2, Fold 5/5\n",
      "Epoch [100/10000], Training Loss: 0.1793, Validation Loss: 0.1482, Validation Accuracy: 0.9339\n",
      "Epoch [200/10000], Training Loss: 0.0953, Validation Loss: 0.1143, Validation Accuracy: 0.9643\n",
      "Early stopping triggered, Epoch [279/10000]\n",
      "Learning Rate 0.01, Dropout Rate 0.5, Fold 1/5\n",
      "Epoch [100/10000], Training Loss: 0.3595, Validation Loss: 0.2424, Validation Accuracy: 0.9109\n",
      "Epoch [200/10000], Training Loss: 0.2466, Validation Loss: 0.1728, Validation Accuracy: 0.9340\n",
      "Epoch [300/10000], Training Loss: 0.2046, Validation Loss: 0.1620, Validation Accuracy: 0.9430\n",
      "Early stopping triggered, Epoch [370/10000]\n",
      "Learning Rate 0.01, Dropout Rate 0.5, Fold 2/5\n",
      "Epoch [100/10000], Training Loss: 0.3768, Validation Loss: 0.2327, Validation Accuracy: 0.9091\n",
      "Epoch [200/10000], Training Loss: 0.2510, Validation Loss: 0.1434, Validation Accuracy: 0.9465\n",
      "Early stopping triggered, Epoch [288/10000]\n",
      "Learning Rate 0.01, Dropout Rate 0.5, Fold 3/5\n",
      "Epoch [100/10000], Training Loss: 0.3455, Validation Loss: 0.2984, Validation Accuracy: 0.8824\n",
      "Epoch [200/10000], Training Loss: 0.2658, Validation Loss: 0.2431, Validation Accuracy: 0.9109\n",
      "Epoch [300/10000], Training Loss: 0.1995, Validation Loss: 0.2246, Validation Accuracy: 0.9234\n",
      "Early stopping triggered, Epoch [399/10000]\n",
      "Learning Rate 0.01, Dropout Rate 0.5, Fold 4/5\n",
      "Epoch [100/10000], Training Loss: 0.3563, Validation Loss: 0.2151, Validation Accuracy: 0.9071\n",
      "Epoch [200/10000], Training Loss: 0.2469, Validation Loss: 0.1598, Validation Accuracy: 0.9339\n",
      "Epoch [300/10000], Training Loss: 0.1883, Validation Loss: 0.1358, Validation Accuracy: 0.9393\n",
      "Early stopping triggered, Epoch [343/10000]\n",
      "Learning Rate 0.01, Dropout Rate 0.5, Fold 5/5\n",
      "Epoch [100/10000], Training Loss: 0.3516, Validation Loss: 0.2544, Validation Accuracy: 0.9018\n",
      "Epoch [200/10000], Training Loss: 0.2591, Validation Loss: 0.1609, Validation Accuracy: 0.9339\n",
      "Epoch [300/10000], Training Loss: 0.1991, Validation Loss: 0.1492, Validation Accuracy: 0.9286\n",
      "Early stopping triggered, Epoch [375/10000]\n",
      "Learning Rate 0.1, Dropout Rate 0.0, Fold 1/5\n",
      "Epoch [100/10000], Training Loss: 0.2880, Validation Loss: 0.3392, Validation Accuracy: 0.8734\n",
      "Epoch [200/10000], Training Loss: 0.0900, Validation Loss: 0.2218, Validation Accuracy: 0.9358\n",
      "Early stopping triggered, Epoch [248/10000]\n",
      "Learning Rate 0.1, Dropout Rate 0.0, Fold 2/5\n",
      "Epoch [100/10000], Training Loss: 0.2249, Validation Loss: 0.2447, Validation Accuracy: 0.9002\n",
      "Early stopping triggered, Epoch [153/10000]\n",
      "Learning Rate 0.1, Dropout Rate 0.0, Fold 3/5\n",
      "Epoch [100/10000], Training Loss: 0.6801, Validation Loss: 0.6953, Validation Accuracy: 0.7059\n",
      "Epoch [200/10000], Training Loss: 0.1311, Validation Loss: 0.3555, Validation Accuracy: 0.9037\n",
      "Early stopping triggered, Epoch [244/10000]\n",
      "Learning Rate 0.1, Dropout Rate 0.0, Fold 4/5\n",
      "Epoch [100/10000], Training Loss: 0.4085, Validation Loss: 0.4629, Validation Accuracy: 0.7964\n",
      "Epoch [200/10000], Training Loss: 0.1741, Validation Loss: 0.2344, Validation Accuracy: 0.9179\n",
      "Epoch [300/10000], Training Loss: 0.0994, Validation Loss: 0.2347, Validation Accuracy: 0.9357\n",
      "Early stopping triggered, Epoch [330/10000]\n",
      "Learning Rate 0.1, Dropout Rate 0.0, Fold 5/5\n",
      "Epoch [100/10000], Training Loss: 0.3943, Validation Loss: 0.4179, Validation Accuracy: 0.8393\n",
      "Epoch [200/10000], Training Loss: 0.3717, Validation Loss: 0.3771, Validation Accuracy: 0.8393\n",
      "Early stopping triggered, Epoch [226/10000]\n",
      "Learning Rate 0.1, Dropout Rate 0.2, Fold 1/5\n",
      "Epoch [100/10000], Training Loss: 0.4480, Validation Loss: 0.3500, Validation Accuracy: 0.8859\n",
      "Epoch [200/10000], Training Loss: 0.2800, Validation Loss: 0.2611, Validation Accuracy: 0.9180\n",
      "Early stopping triggered, Epoch [259/10000]\n",
      "Learning Rate 0.1, Dropout Rate 0.2, Fold 2/5\n",
      "Epoch [100/10000], Training Loss: 0.3423, Validation Loss: 0.2635, Validation Accuracy: 0.9109\n",
      "Epoch [200/10000], Training Loss: 0.2191, Validation Loss: 0.1847, Validation Accuracy: 0.9269\n",
      "Early stopping triggered, Epoch [202/10000]\n",
      "Learning Rate 0.1, Dropout Rate 0.2, Fold 3/5\n",
      "Epoch [100/10000], Training Loss: 0.3226, Validation Loss: 0.2844, Validation Accuracy: 0.8788\n",
      "Epoch [200/10000], Training Loss: 0.2221, Validation Loss: 0.2692, Validation Accuracy: 0.9020\n",
      "Early stopping triggered, Epoch [258/10000]\n",
      "Learning Rate 0.1, Dropout Rate 0.2, Fold 4/5\n",
      "Epoch [100/10000], Training Loss: 0.2237, Validation Loss: 0.2005, Validation Accuracy: 0.9196\n",
      "Epoch [200/10000], Training Loss: 0.1869, Validation Loss: 0.1582, Validation Accuracy: 0.9411\n",
      "Early stopping triggered, Epoch [261/10000]\n",
      "Learning Rate 0.1, Dropout Rate 0.2, Fold 5/5\n",
      "Epoch [100/10000], Training Loss: 0.7448, Validation Loss: 0.6843, Validation Accuracy: 0.6000\n",
      "Epoch [200/10000], Training Loss: 0.5752, Validation Loss: 0.5163, Validation Accuracy: 0.7893\n",
      "Epoch [300/10000], Training Loss: 0.5088, Validation Loss: 0.5346, Validation Accuracy: 0.8839\n",
      "Early stopping triggered, Epoch [329/10000]\n",
      "Learning Rate 0.1, Dropout Rate 0.5, Fold 1/5\n",
      "Epoch [100/10000], Training Loss: 0.6699, Validation Loss: 0.5986, Validation Accuracy: 0.6970\n",
      "Epoch [200/10000], Training Loss: 0.6408, Validation Loss: 0.4921, Validation Accuracy: 0.8217\n",
      "Epoch [300/10000], Training Loss: 0.5278, Validation Loss: 0.3877, Validation Accuracy: 0.8806\n",
      "Early stopping triggered, Epoch [369/10000]\n",
      "Learning Rate 0.1, Dropout Rate 0.5, Fold 2/5\n",
      "Epoch [100/10000], Training Loss: 0.8036, Validation Loss: 0.6759, Validation Accuracy: 0.5847\n",
      "Epoch [200/10000], Training Loss: 0.7053, Validation Loss: 0.5318, Validation Accuracy: 0.7897\n",
      "Epoch [300/10000], Training Loss: 0.6922, Validation Loss: 0.4825, Validation Accuracy: 0.8164\n",
      "Early stopping triggered, Epoch [339/10000]\n",
      "Learning Rate 0.1, Dropout Rate 0.5, Fold 3/5\n",
      "Early stopping triggered, Epoch [59/10000]\n",
      "Learning Rate 0.1, Dropout Rate 0.5, Fold 4/5\n",
      "Epoch [100/10000], Training Loss: 0.7543, Validation Loss: 0.6791, Validation Accuracy: 0.5786\n",
      "Epoch [200/10000], Training Loss: 0.6802, Validation Loss: 0.6117, Validation Accuracy: 0.7000\n",
      "Epoch [300/10000], Training Loss: 0.6553, Validation Loss: 0.5306, Validation Accuracy: 0.8107\n",
      "Epoch [400/10000], Training Loss: 0.6461, Validation Loss: 0.4839, Validation Accuracy: 0.8571\n",
      "Epoch [500/10000], Training Loss: 0.6055, Validation Loss: 0.3997, Validation Accuracy: 0.8875\n",
      "Early stopping triggered, Epoch [597/10000]\n",
      "Learning Rate 0.1, Dropout Rate 0.5, Fold 5/5\n",
      "Epoch [100/10000], Training Loss: 1.0112, Validation Loss: 0.9447, Validation Accuracy: 0.5232\n",
      "Epoch [200/10000], Training Loss: 0.9330, Validation Loss: 0.8460, Validation Accuracy: 0.6446\n",
      "Epoch [300/10000], Training Loss: 0.9124, Validation Loss: 0.8063, Validation Accuracy: 0.6679\n",
      "Early stopping triggered, Epoch [335/10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25441bcc8d842749e7646381efe61bf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>▁▂▃▃▁▃▂▃█</td></tr><tr><td>avg_val_accuracy</td><td>██▇▆██▇▇▁</td></tr><tr><td>avg_val_loss</td><td>▁▁▁▃▁▁▃▂█</td></tr><tr><td>dropout_rate</td><td>▁▄█▁▄█▁▄█</td></tr><tr><td>learning_rate</td><td>▁▁▁▂▂▂███</td></tr><tr><td>std_train_loss</td><td>▁▁▁▄▁▁▂▃█</td></tr><tr><td>std_val_accuracy</td><td>▁▁▁▄▁▁▂▃█</td></tr><tr><td>std_val_loss</td><td>▁▁▁▄▁▁▂▃█</td></tr><tr><td>total_epochs</td><td>▁▇█▁▂▄▂▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_train_loss</td><td>0.7697</td></tr><tr><td>avg_val_accuracy</td><td>0.69288</td></tr><tr><td>avg_val_loss</td><td>0.65699</td></tr><tr><td>dropout_rate</td><td>0.5</td></tr><tr><td>learning_rate</td><td>0.1</td></tr><tr><td>std_train_loss</td><td>0.2573</td></tr><tr><td>std_val_accuracy</td><td>0.2573</td></tr><tr><td>std_val_loss</td><td>0.2573</td></tr><tr><td>total_epochs</td><td>335</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">misty-dream-8</strong> at: <a href='https://wandb.ai/u21432962-university-of-pretoria/COS%20711_Assignment%202/runs/o7nmtpuy' target=\"_blank\">https://wandb.ai/u21432962-university-of-pretoria/COS%20711_Assignment%202/runs/o7nmtpuy</a><br/> View project at: <a href='https://wandb.ai/u21432962-university-of-pretoria/COS%20711_Assignment%202' target=\"_blank\">https://wandb.ai/u21432962-university-of-pretoria/COS%20711_Assignment%202</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240921_023908-o7nmtpuy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Grid search with k fold cross validation using seed 123'''\n",
    "\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "dropout_rates = [0.0, 0.2, 0.5]\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"COS711_Assignment_2.ipynb\"\n",
    "wandb.init(project=\"COS 711_Assignment 2\")\n",
    "\n",
    "set_seed(123)\n",
    "grid_search(learning_rates, dropout_rates, input_size, output_size, x, y_encoded, num_epochs, k_folds=5)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_tensor size: torch.Size([2018, 12])\n",
      "y_train_tensor size: torch.Size([2018])\n",
      "x_val_tensor size: torch.Size([505, 12])\n",
      "y_val_tensor size: torch.Size([505])\n",
      "x_test_tensor size: torch.Size([280, 12])\n",
      "y_test_tensor size: torch.Size([280])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Train, validation and test set splitting\"\"\"\n",
    "\n",
    "set_seed(random_seed)\n",
    "\n",
    "\n",
    "x_test, x_temp, y_test, y_temp = train_test_split(\n",
    "    x, y_encoded, train_size=0.1, random_state=1\n",
    ")\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_temp, y_temp, train_size=0.8, random_state=random_seed\n",
    ")\n",
    "\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "x_val_tensor = torch.tensor(x_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "print(\"x_train_tensor size:\", x_train_tensor.size())\n",
    "print(\"y_train_tensor size:\", y_train_tensor.size())\n",
    "print(\"x_val_tensor size:\", x_val_tensor.size())\n",
    "print(\"y_val_tensor size:\", y_val_tensor.size())\n",
    "print(\"x_test_tensor size:\", x_test_tensor.size())\n",
    "print(\"y_test_tensor size:\", y_test_tensor.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(input_size, output_size, dropout_rate)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=50, delta=0.001)\n",
    "\n",
    "trainer(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    early_stopping,\n",
    "    x_train_tensor,\n",
    "    y_train_tensor,\n",
    "    x_val_tensor,\n",
    "    y_val_tensor,\n",
    "    num_epochs,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
