{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "import yaml\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Classes\"\"\"\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_rate):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.ln1 = nn.LayerNorm(256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.ln2 = nn.LayerNorm(128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.ln3 = nn.LayerNorm(64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.ln4 = nn.LayerNorm(32)\n",
    "        self.output_layer = nn.Linear(32, output_size)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.ln1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.ln2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.ln3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.ln4(self.fc4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "\n",
    "class HybridOptimizer:\n",
    "    def __init__(self, optimizers):\n",
    "        self.optimizers = optimizers\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for opt in self.optimizers:\n",
    "            opt.zero_grad()\n",
    "\n",
    "    def average_step(self):\n",
    "        \n",
    "        grad_accum = {}\n",
    "        num_opts = len(self.optimizers)\n",
    "\n",
    "        for opt in self.optimizers:\n",
    "            for group in opt.param_groups:\n",
    "                for param in group[\"params\"]:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    if param in grad_accum:\n",
    "                        grad_accum[param] += param.grad.detach().clone()\n",
    "                    else:\n",
    "                        grad_accum[param] = param.grad.detach().clone()\n",
    "\n",
    "        for param in grad_accum:\n",
    "            grad_accum[param] /= num_opts\n",
    "            param.grad = grad_accum[param]\n",
    "\n",
    "        for opt in self.optimizers:\n",
    "            opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Functions\"\"\"\n",
    "\n",
    "\n",
    "def plot_feature_distributions(data, data_imputed, features):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, feature in enumerate(features):\n",
    "        plt.subplot(4, 3, i + 1)\n",
    "        sns.histplot(\n",
    "            data[feature].dropna(),\n",
    "            kde=True,\n",
    "            label=\"Before Imputation\",\n",
    "            color=\"blue\",\n",
    "            bins=30,\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        sns.histplot(\n",
    "            data_imputed[feature].dropna(),\n",
    "            kde=True,\n",
    "            label=\"After Imputation\",\n",
    "            color=\"red\",\n",
    "            bins=30,\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        plt.title(f\"{feature}\")\n",
    "        plt.xlabel(\"\")\n",
    "        plt.ylabel(\"\")\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_box_plots_comparison(data, data_imputed, features):\n",
    "    plt.figure(figsize=(15, 12))\n",
    "\n",
    "    for i, feature in enumerate(features):\n",
    "        plt.subplot(4, 3, i + 1)\n",
    "\n",
    "        df_before_plot = data[[feature]].copy()\n",
    "        df_before_plot[\"Imputation Status\"] = \"Before Imputation\"\n",
    "\n",
    "        df_after_plot = data_imputed[[feature]].copy()\n",
    "        df_after_plot[\"Imputation Status\"] = \"After Imputation\"\n",
    "\n",
    "        df_plot = pd.concat([df_before_plot, df_after_plot], ignore_index=True)\n",
    "\n",
    "        sns.boxplot(\n",
    "            x=\"Imputation Status\",\n",
    "            y=feature,\n",
    "            data=df_plot,\n",
    "            hue=\"Imputation Status\",\n",
    "            palette=\"Set2\",\n",
    "            showfliers=True,\n",
    "        )\n",
    "\n",
    "        plt.title(f\"{feature}\")\n",
    "        plt.xlabel(\"\")\n",
    "        plt.ylabel(\"\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_correlation_heatmaps(data, data_imputed, features):\n",
    "    corr_before = data[features].corr()\n",
    "    corr_after = data_imputed[features].corr()\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(corr_before, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    plt.title(\"Correlation Before Imputation\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.heatmap(corr_after, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    plt.title(\"Correlation After Imputation\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def single_plot_feature_distributions(data, features):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, feature in enumerate(features):\n",
    "        plt.subplot(4, 3, i + 1)\n",
    "        sns.histplot(\n",
    "            data[feature].dropna(),\n",
    "            kde=True,\n",
    "            label=\"Normalized\",\n",
    "            color=\"blue\",\n",
    "            bins=30,\n",
    "            alpha=1,\n",
    "        )\n",
    "        plt.title(f\"{feature}\")\n",
    "        plt.xlabel(\"\")\n",
    "        plt.ylabel(\"\")\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def impute_features(data, target):\n",
    "    impute_features = [\n",
    "        \"Length (major axis)\",\n",
    "        \"Width (minor axis)\",\n",
    "        \"Thickness (depth)\",\n",
    "        \"Area\",\n",
    "        \"Perimeter\",\n",
    "        \"Solidity\",\n",
    "        \"Compactness\",\n",
    "        \"Extent\",\n",
    "        \"Convex hull(convex area)\",\n",
    "    ]\n",
    "\n",
    "    knn_imputer = KNNImputer(n_neighbors=53)  # n_neighbors = root of dataset size\n",
    "\n",
    "    data_imputed = data.copy()\n",
    "\n",
    "    types = data[target].unique()\n",
    "    for almond_type in types:\n",
    "\n",
    "        type_data = data[data[target] == almond_type].copy()\n",
    "\n",
    "        type_features = type_data[impute_features]\n",
    "\n",
    "        imputed_values = knn_imputer.fit_transform(type_features)\n",
    "\n",
    "        type_data[impute_features] = imputed_values\n",
    "\n",
    "        data_imputed.update(type_data)\n",
    "\n",
    "    data_imputed[\"Roundness\"] = (4 * data_imputed[\"Area\"]) / (\n",
    "        np.pi * data_imputed[\"Length (major axis)\"] ** 2\n",
    "    )\n",
    "\n",
    "    data_imputed[\"Aspect Ratio\"] = (\n",
    "        data_imputed[\"Length (major axis)\"] / data_imputed[\"Width (minor axis)\"]\n",
    "    )\n",
    "\n",
    "    data_imputed[\"Eccentricity\"] = np.sqrt(\n",
    "        1\n",
    "        - (data_imputed[\"Width (minor axis)\"] / data_imputed[\"Length (major axis)\"])\n",
    "        ** 2\n",
    "    )\n",
    "\n",
    "    return data_imputed\n",
    "\n",
    "\n",
    "def cap_outliers_percentiles(data, feature, lower_percentile=5, upper_percentile=95):\n",
    "    lower_limit = np.percentile(data[feature], lower_percentile)\n",
    "    upper_limit = np.percentile(data[feature], upper_percentile)\n",
    "\n",
    "    data[feature] = np.where(data[feature] < lower_limit, lower_limit, data[feature])\n",
    "    data[feature] = np.where(data[feature] > upper_limit, upper_limit, data[feature])\n",
    "\n",
    "\n",
    "def normalize_data(data, features):\n",
    "    scaler = StandardScaler()\n",
    "    normalized_data = data.copy()\n",
    "    normalized_data[features] = scaler.fit_transform(data[features])\n",
    "    return normalized_data\n",
    "\n",
    "\n",
    "def trainer(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    early_stopping,\n",
    "    x_train_tensor,\n",
    "    y_train_tensor,\n",
    "    x_val_tensor,\n",
    "    y_val_tensor,\n",
    "    num_epochs,\n",
    "    hybrid,\n",
    "    graph,\n",
    "):\n",
    "\n",
    "    if graph:\n",
    "        wandb.init(project=\"COS 711_Assignment 2\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        outputs = model(x_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        if hybrid and isinstance(optimizer, HybridOptimizer):\n",
    "            grad_info = {}\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    grad_info[name] = param.grad.clone().cpu().numpy()\n",
    "\n",
    "            optimizer.average_step()\n",
    "        else:\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = loss.item()\n",
    "\n",
    "        val_accuracy, val_loss = evaluate_model(\n",
    "            model, x_val_tensor, y_val_tensor, criterion\n",
    "        )\n",
    "\n",
    "        if graph:\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"Epoch\": epoch,\n",
    "                    \"Training Loss\": train_loss,\n",
    "                    \"Validation Loss\": val_loss,\n",
    "                    \"Validation Accuracy\": val_accuracy,\n",
    "                    \"Optimizer\": optimizer.__class__.__name__,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch + 1}/{num_epochs}], Training Loss: {train_loss:.4f}, \"\n",
    "                f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, \"\n",
    "                f\"Optimizer: {optimizer.__class__.__name__}\"\n",
    "            )\n",
    "\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            wandb.finish()\n",
    "            print(f\"Early stopping triggered, Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "\n",
    "            return train_loss, val_loss, val_accuracy, (epoch + 1)\n",
    "\n",
    "\n",
    "def evaluate_model(model, x_tensor, y_tensor, criterion):\n",
    "    model.eval()\n",
    "    total_samples = len(y_tensor)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(x_tensor)\n",
    "        loss = criterion(outputs, y_tensor).item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        accuracy = (predicted == y_tensor).sum().item() / total_samples\n",
    "\n",
    "    return accuracy, loss\n",
    "\n",
    "\n",
    "def grid_search(\n",
    "    optimizer,\n",
    "    learning_rates,\n",
    "    dropout_rates,\n",
    "    input_size,\n",
    "    output_size,\n",
    "    x,\n",
    "    y_encoded,\n",
    "    num_epochs,\n",
    "    k_folds,\n",
    "    hybrid,\n",
    "):\n",
    "    date_time_str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    if hybrid:\n",
    "        optimizer_name = \"HybridOptimizer\"\n",
    "    else:\n",
    "        optimizer_name = optimizer.__name__\n",
    "    filename = f\"results/grid_search_optimizer_{optimizer_name}_{date_time_str}.csv\"\n",
    "    for lr in learning_rates:\n",
    "        for dr in dropout_rates:\n",
    "            kf = KFold(n_splits=k_folds, shuffle=True, random_state=123)\n",
    "            fold_train_losses = []\n",
    "            fold_val_losses = []\n",
    "            fold_val_accuracies = []\n",
    "            fold_total_epochs = []\n",
    "\n",
    "            for fold, (train_idx, val_idx) in enumerate(kf.split(x)):\n",
    "                set_seed(fold)\n",
    "\n",
    "                print(\n",
    "                    f\"Learning Rate {lr}, Dropout Rate {dr}, Fold {fold + 1}/{k_folds}\"\n",
    "                )\n",
    "\n",
    "                x_train, x_val = x[train_idx], x[val_idx]\n",
    "                y_train, y_val = y_encoded[train_idx], y_encoded[val_idx]\n",
    "\n",
    "                x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "                y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "                x_val_tensor = torch.tensor(x_val, dtype=torch.float32)\n",
    "                y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "                model = MLP(input_size, output_size, dr)\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "                if hybrid:\n",
    "                    hybrid_optimizers = [\n",
    "                        optimizer[\"sgd\"](model.parameters(), lr=lr),\n",
    "                        optimizer[\"adam\"](model.parameters(), lr=lr),\n",
    "                        optimizer[\"rprop\"](model.parameters()),\n",
    "                    ]\n",
    "                    active_optimizer = HybridOptimizer(hybrid_optimizers)\n",
    "                else:\n",
    "                    active_optimizer = optimizer(model.parameters(), lr=lr)\n",
    "\n",
    "                early_stopping = EarlyStopping(patience=50, delta=0.001)\n",
    "\n",
    "                (\n",
    "                    final_train_loss,\n",
    "                    final_val_loss,\n",
    "                    final_val_accuracy,\n",
    "                    total_epochs,\n",
    "                    gradient_data,\n",
    "                ) = trainer(\n",
    "                    model,\n",
    "                    criterion,\n",
    "                    active_optimizer,\n",
    "                    early_stopping,\n",
    "                    x_train_tensor,\n",
    "                    y_train_tensor,\n",
    "                    x_val_tensor,\n",
    "                    y_val_tensor,\n",
    "                    num_epochs,\n",
    "                    hybrid,\n",
    "                    False,\n",
    "                )\n",
    "\n",
    "                print(total_epochs)\n",
    "\n",
    "                fold_train_losses.append(final_train_loss)\n",
    "                fold_val_losses.append(final_val_loss)\n",
    "                fold_val_accuracies.append(final_val_accuracy)\n",
    "                fold_total_epochs.append(total_epochs)\n",
    "\n",
    "            avg_train_loss = np.mean(fold_train_losses)\n",
    "            avg_val_loss = np.mean(fold_val_losses)\n",
    "            avg_val_accuracy = np.mean(fold_val_accuracies)\n",
    "            avg_total_epochs = np.mean(fold_total_epochs)\n",
    "            std_train_loss = np.std(fold_train_losses)\n",
    "            std_val_loss = np.std(fold_val_losses)\n",
    "            std_val_accuracy = np.std(fold_val_losses)\n",
    "            std_total_epochs = np.std(fold_total_epochs)\n",
    "\n",
    "            save_grid_search_results(\n",
    "                lr,\n",
    "                dr,\n",
    "                avg_train_loss,\n",
    "                avg_val_loss,\n",
    "                avg_val_accuracy,\n",
    "                avg_total_epochs,\n",
    "                std_train_loss,\n",
    "                std_val_loss,\n",
    "                std_val_accuracy,\n",
    "                std_total_epochs,\n",
    "                filename,\n",
    "            )\n",
    "\n",
    "\n",
    "def train_test(\n",
    "    x_test,\n",
    "    x_train,\n",
    "    y_test,\n",
    "    y_train,\n",
    "    input_size,\n",
    "    output_size,\n",
    "    num_tests,\n",
    "    optimizer,\n",
    "    num_epochs,\n",
    "    hybrid,\n",
    "    lr,\n",
    "    dr,\n",
    "    filename,\n",
    "):\n",
    "    fold_train_losses = []\n",
    "    fold_val_losses = []\n",
    "    fold_val_accuracies = []\n",
    "    fold_test_accuracies = []\n",
    "    fold_total_epochs = []\n",
    "\n",
    "    for _ in range(num_tests):\n",
    "        model = MLP(input_size, output_size, dr)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        if hybrid:\n",
    "            hybrid_optimizers = [\n",
    "                optimizer[\"sgd\"](model.parameters(), lr=lr),\n",
    "                optimizer[\"adam\"](model.parameters(), lr=lr),\n",
    "                optimizer[\"rprop\"](model.parameters()),\n",
    "            ]\n",
    "            active_optimizer = HybridOptimizer(hybrid_optimizers)\n",
    "        else:\n",
    "            active_optimizer = optimizer(model.parameters(), lr=lr)\n",
    "\n",
    "        early_stopping = EarlyStopping(patience=50, delta=0.001)\n",
    "\n",
    "        random_seed = random.randint(0, 100000)\n",
    "\n",
    "        run_x_train, run_x_val, run_y_train, run_y_val = train_test_split(\n",
    "            x_train, y_train, train_size=0.8, random_state=random_seed\n",
    "        )\n",
    "\n",
    "        x_train_tensor = torch.tensor(run_x_train, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(run_y_train, dtype=torch.long)\n",
    "        x_val_tensor = torch.tensor(run_x_val, dtype=torch.float32)\n",
    "        y_val_tensor = torch.tensor(run_y_val, dtype=torch.long)\n",
    "        x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "        (\n",
    "            final_train_loss,\n",
    "            final_val_loss,\n",
    "            final_val_accuracy,\n",
    "            total_epochs,\n",
    "        ) = trainer(\n",
    "            model,\n",
    "            criterion,\n",
    "            active_optimizer,\n",
    "            early_stopping,\n",
    "            x_train_tensor,\n",
    "            y_train_tensor,\n",
    "            x_val_tensor,\n",
    "            y_val_tensor,\n",
    "            num_epochs,\n",
    "            hybrid,\n",
    "            True,\n",
    "        )\n",
    "\n",
    "        test_accuracy, loss = evaluate_model(\n",
    "            model, x_test_tensor, y_test_tensor, criterion\n",
    "        )\n",
    "\n",
    "        fold_train_losses.append(final_train_loss)\n",
    "        fold_val_losses.append(final_val_loss)\n",
    "        fold_val_accuracies.append(final_val_accuracy)\n",
    "        fold_test_accuracies.append(test_accuracy)\n",
    "        fold_total_epochs.append(total_epochs)\n",
    "\n",
    "    avg_train_loss = np.mean(fold_train_losses)\n",
    "    avg_val_loss = np.mean(fold_val_losses)\n",
    "    avg_val_accuracy = np.mean(fold_val_accuracies)\n",
    "    avg_test_accuracy = np.mean(fold_test_accuracies)\n",
    "    avg_total_epochs = np.mean(fold_total_epochs)\n",
    "    std_train_loss = np.std(fold_train_losses)\n",
    "    std_val_loss = np.std(fold_val_losses)\n",
    "    std_val_accuracy = np.std(fold_val_losses)\n",
    "    std_test_accuracy = np.std(fold_test_accuracies)\n",
    "    std_total_epochs = np.std(fold_total_epochs)\n",
    "\n",
    "    save_results(\n",
    "        lr,\n",
    "        dr,\n",
    "        avg_train_loss,\n",
    "        avg_val_loss,\n",
    "        avg_val_accuracy,\n",
    "        avg_test_accuracy,\n",
    "        avg_total_epochs,\n",
    "        std_train_loss,\n",
    "        std_val_loss,\n",
    "        std_val_accuracy,\n",
    "        std_total_epochs,\n",
    "        std_test_accuracy,\n",
    "        filename,\n",
    "    )\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "def save_grid_search_results(\n",
    "    lr,\n",
    "    dr,\n",
    "    avg_train_loss,\n",
    "    avg_val_loss,\n",
    "    avg_val_accuracy,\n",
    "    avg_total_epochs,\n",
    "    std_train_loss,\n",
    "    std_val_loss,\n",
    "    std_val_accuracy,\n",
    "    std_total_epochs,\n",
    "    filename,\n",
    "):\n",
    "\n",
    "    header = [\n",
    "        \"learning_rate\",\n",
    "        \"dropout_rate\",\n",
    "        \"avg_train_loss\",\n",
    "        \"avg_val_loss\",\n",
    "        \"avg_val_accuracy\",\n",
    "        \"avg_total_epochs\",\n",
    "        \"std_train_loss\",\n",
    "        \"std_val_loss\",\n",
    "        \"std_val_accuracy\",\n",
    "        \"std_total_epochs\",\n",
    "    ]\n",
    "\n",
    "    row = [\n",
    "        lr,\n",
    "        dr,\n",
    "        avg_train_loss,\n",
    "        avg_val_loss,\n",
    "        avg_val_accuracy,\n",
    "        avg_total_epochs,\n",
    "        std_train_loss,\n",
    "        std_val_loss,\n",
    "        std_val_accuracy,\n",
    "        std_total_epochs,\n",
    "    ]\n",
    "\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    with open(filename, mode=\"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        if not file_exists:\n",
    "            writer.writerow(header)\n",
    "        writer.writerow(row)\n",
    "\n",
    "    print(f\"Results saved to {filename}\")\n",
    "\n",
    "\n",
    "def save_results(\n",
    "    lr,\n",
    "    dr,\n",
    "    avg_train_loss,\n",
    "    avg_val_loss,\n",
    "    avg_val_accuracy,\n",
    "    avg_test_accuracy,\n",
    "    avg_total_epochs,\n",
    "    std_train_loss,\n",
    "    std_val_loss,\n",
    "    std_val_accuracy,\n",
    "    std_total_epochs,\n",
    "    std_test_accuracy,\n",
    "    filename,\n",
    "):\n",
    "\n",
    "    header = [\n",
    "        \"learning_rate\",\n",
    "        \"dropout_rate\",\n",
    "        \"avg_train_loss\",\n",
    "        \"avg_val_loss\",\n",
    "        \"avg_val_accuracy\",\n",
    "        \"avg_test_accuracy\",\n",
    "        \"avg_total_epochs\",\n",
    "        \"std_train_loss\",\n",
    "        \"std_val_loss\",\n",
    "        \"std_val_accuracy\",\n",
    "        \"std_total_epochs\",\n",
    "        \"std_test_accuracy\",\n",
    "    ]\n",
    "\n",
    "    row = [\n",
    "        lr,\n",
    "        dr,\n",
    "        avg_train_loss,\n",
    "        avg_val_loss,\n",
    "        avg_val_accuracy,\n",
    "        avg_test_accuracy,\n",
    "        avg_total_epochs,\n",
    "        std_train_loss,\n",
    "        std_val_loss,\n",
    "        std_val_accuracy,\n",
    "        std_total_epochs,\n",
    "        std_test_accuracy,\n",
    "    ]\n",
    "\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    with open(filename, mode=\"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        if not file_exists:\n",
    "            writer.writerow(header)\n",
    "        writer.writerow(row)\n",
    "\n",
    "    print(f\"Results saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"Configuration\"\"\"\n",
    "\n",
    "with open(\"config.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "random_seed = random.randint(0, 100000)\n",
    "\n",
    "print_graphs = config[\"print_graphs\"]\n",
    "perform_grid_search = config[\"perform_grid_search\"]\n",
    "features = config[\"features\"]\n",
    "target = config[\"target\"]\n",
    "num_epochs = config[\"num_epochs\"]\n",
    "learning_rate = config[\"learning_rate\"]\n",
    "dropout_rate = config[\"dropout_rate\"]\n",
    "learning_rates = config[\"learning_rates\"]\n",
    "dropout_rates = config[\"dropout_rates\"]\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data setup\"\"\"\n",
    "\n",
    "data = pd.read_csv(\"data/almond_data.csv\")\n",
    "\n",
    "data = data.iloc[:, 1:]\n",
    "\n",
    "data.head()\n",
    "\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "data_imputed = impute_features(data, target)\n",
    "\n",
    "for feature in features:\n",
    "    cap_outliers_percentiles(data_imputed, feature)\n",
    "\n",
    "if print_graphs:\n",
    "    plot_feature_distributions(data, data_imputed, features)\n",
    "    plot_box_plots_comparison(data, data_imputed, features)\n",
    "    plot_correlation_heatmaps(data, data_imputed, features)\n",
    "\n",
    "normalized_data = normalize_data(data_imputed, features)\n",
    "\n",
    "if print_graphs:\n",
    "    single_plot_feature_distributions(normalized_data, features)\n",
    "\n",
    "x = normalized_data[features].values\n",
    "y = normalized_data[target].values\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "input_size = len(features)\n",
    "output_size = len(label_encoder.classes_)\n",
    "\n",
    "x_test, x_train, y_test, y_train = train_test_split(\n",
    "    x, y_encoded, train_size=0.2, random_state=123\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.0001, Dropout Rate 0.0, Fold 1/5\n",
      "Epoch [50/100000], Training Loss: 1.0790, Validation Loss: 1.1086, Validation Accuracy: 0.2472, Optimizer: SGD\n",
      "Epoch [100/100000], Training Loss: 1.0648, Validation Loss: 1.0927, Validation Accuracy: 0.2851, Optimizer: SGD\n",
      "Epoch [150/100000], Training Loss: 1.0520, Validation Loss: 1.0786, Validation Accuracy: 0.3318, Optimizer: SGD\n",
      "Epoch [200/100000], Training Loss: 1.0404, Validation Loss: 1.0659, Validation Accuracy: 0.3653, Optimizer: SGD\n",
      "Epoch [250/100000], Training Loss: 1.0297, Validation Loss: 1.0544, Validation Accuracy: 0.4031, Optimizer: SGD\n",
      "Epoch [300/100000], Training Loss: 1.0199, Validation Loss: 1.0439, Validation Accuracy: 0.4365, Optimizer: SGD\n",
      "Epoch [350/100000], Training Loss: 1.0109, Validation Loss: 1.0342, Validation Accuracy: 0.4588, Optimizer: SGD\n",
      "Epoch [400/100000], Training Loss: 1.0025, Validation Loss: 1.0252, Validation Accuracy: 0.4944, Optimizer: SGD\n",
      "Epoch [450/100000], Training Loss: 0.9945, Validation Loss: 1.0169, Validation Accuracy: 0.5078, Optimizer: SGD\n",
      "Epoch [500/100000], Training Loss: 0.9868, Validation Loss: 1.0090, Validation Accuracy: 0.5278, Optimizer: SGD\n",
      "Epoch [550/100000], Training Loss: 0.9796, Validation Loss: 1.0017, Validation Accuracy: 0.5301, Optimizer: SGD\n",
      "Epoch [600/100000], Training Loss: 0.9728, Validation Loss: 0.9947, Validation Accuracy: 0.5301, Optimizer: SGD\n",
      "Epoch [650/100000], Training Loss: 0.9663, Validation Loss: 0.9882, Validation Accuracy: 0.5457, Optimizer: SGD\n",
      "Epoch [700/100000], Training Loss: 0.9602, Validation Loss: 0.9821, Validation Accuracy: 0.5457, Optimizer: SGD\n",
      "Epoch [750/100000], Training Loss: 0.9543, Validation Loss: 0.9763, Validation Accuracy: 0.5501, Optimizer: SGD\n",
      "Epoch [800/100000], Training Loss: 0.9487, Validation Loss: 0.9707, Validation Accuracy: 0.5546, Optimizer: SGD\n",
      "Epoch [850/100000], Training Loss: 0.9433, Validation Loss: 0.9655, Validation Accuracy: 0.5635, Optimizer: SGD\n",
      "Epoch [900/100000], Training Loss: 0.9382, Validation Loss: 0.9604, Validation Accuracy: 0.5768, Optimizer: SGD\n",
      "Epoch [950/100000], Training Loss: 0.9332, Validation Loss: 0.9556, Validation Accuracy: 0.5880, Optimizer: SGD\n",
      "Epoch [1000/100000], Training Loss: 0.9283, Validation Loss: 0.9510, Validation Accuracy: 0.5969, Optimizer: SGD\n",
      "Epoch [1050/100000], Training Loss: 0.9236, Validation Loss: 0.9467, Validation Accuracy: 0.6013, Optimizer: SGD\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"Grid search with k fold cross validation using seed 123\"\"\"\n",
    "if perform_grid_search:\n",
    "\n",
    "    set_seed(123)\n",
    "    \n",
    "    \"\"\"SGD\"\"\"\n",
    "    \n",
    "    optimizer = optim.SGD\n",
    "\n",
    "    grid_search(\n",
    "        optimizer,\n",
    "        learning_rates,\n",
    "        dropout_rates,\n",
    "        input_size,\n",
    "        output_size,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        num_epochs,\n",
    "        k_folds=5,\n",
    "        hybrid=False,\n",
    "    )\n",
    "    \n",
    "    \"\"\"Adam\"\"\"\n",
    "    \n",
    "    set_seed(123)\n",
    "    \n",
    "    optimizer = optim.Adam\n",
    "\n",
    "    grid_search(\n",
    "        optimizer,\n",
    "        learning_rates,\n",
    "        dropout_rates,\n",
    "        input_size,\n",
    "        output_size,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        num_epochs,\n",
    "        k_folds=5,\n",
    "        hybrid=False,\n",
    "    )\n",
    "    \n",
    "    \"\"\"Rprop\"\"\"\n",
    "    \n",
    "    set_seed(123)\n",
    "    \n",
    "    optimizer = optim.Rprop\n",
    "\n",
    "    grid_search(\n",
    "        optimizer,\n",
    "        learning_rates,\n",
    "        dropout_rates,\n",
    "        input_size,\n",
    "        output_size,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        num_epochs,\n",
    "        k_folds=5,\n",
    "        hybrid=False,\n",
    "    )\n",
    "    \n",
    "    \"\"\"Hybrid\"\"\"\n",
    "    \n",
    "    set_seed(123)\n",
    "    \n",
    "    hybrid_optimizers = {\"sgd\": optim.SGD, \"adam\": optim.Adam, \"rprop\": optim.Rprop}\n",
    "\n",
    "    grid_search(\n",
    "        hybrid_optimizers,\n",
    "        learning_rates,\n",
    "        dropout_rates,\n",
    "        input_size,\n",
    "        output_size,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        num_epochs,\n",
    "        k_folds=5,\n",
    "        hybrid=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mu21432962\u001b[0m (\u001b[33mu21432962-university-of-pretoria\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/oliver_welsh/Oliver_Welsh/Code/GitHub/COS711_Assignment_2/wandb/run-20241004_222710-f26aufgf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/u21432962-university-of-pretoria/COS%20711_Assignment%202/runs/f26aufgf' target=\"_blank\">polished-energy-241</a></strong> to <a href='https://wandb.ai/u21432962-university-of-pretoria/COS%20711_Assignment%202' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/u21432962-university-of-pretoria/COS%20711_Assignment%202' target=\"_blank\">https://wandb.ai/u21432962-university-of-pretoria/COS%20711_Assignment%202</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/u21432962-university-of-pretoria/COS%20711_Assignment%202/runs/f26aufgf' target=\"_blank\">https://wandb.ai/u21432962-university-of-pretoria/COS%20711_Assignment%202/runs/f26aufgf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/100000], Training Loss: 0.9295, Validation Loss: 0.8720, Validation Accuracy: 0.6837, Optimizer: SGD\n",
      "Epoch [100/100000], Training Loss: 0.8345, Validation Loss: 0.7642, Validation Accuracy: 0.7127, Optimizer: SGD\n",
      "Epoch [150/100000], Training Loss: 0.7651, Validation Loss: 0.6947, Validation Accuracy: 0.7439, Optimizer: SGD\n",
      "Epoch [200/100000], Training Loss: 0.7016, Validation Loss: 0.6349, Validation Accuracy: 0.7706, Optimizer: SGD\n",
      "Epoch [250/100000], Training Loss: 0.6475, Validation Loss: 0.5849, Validation Accuracy: 0.8040, Optimizer: SGD\n",
      "Epoch [300/100000], Training Loss: 0.5963, Validation Loss: 0.5436, Validation Accuracy: 0.8196, Optimizer: SGD\n",
      "Epoch [350/100000], Training Loss: 0.5739, Validation Loss: 0.5105, Validation Accuracy: 0.8307, Optimizer: SGD\n",
      "Epoch [400/100000], Training Loss: 0.5434, Validation Loss: 0.4824, Validation Accuracy: 0.8307, Optimizer: SGD\n",
      "Epoch [450/100000], Training Loss: 0.5250, Validation Loss: 0.4581, Validation Accuracy: 0.8352, Optimizer: SGD\n",
      "Epoch [500/100000], Training Loss: 0.4960, Validation Loss: 0.4368, Validation Accuracy: 0.8463, Optimizer: SGD\n",
      "Epoch [550/100000], Training Loss: 0.4779, Validation Loss: 0.4159, Validation Accuracy: 0.8575, Optimizer: SGD\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m set_seed(\u001b[38;5;241m123\u001b[39m)\n\u001b[1;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtrain_test\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSGD_test_results_best\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m\"\"\"Adam best\"\"\"\u001b[39;00m\n\u001b[1;32m     26\u001b[0m set_seed(\u001b[38;5;241m123\u001b[39m)\n",
      "Cell \u001b[0;32mIn[34], line 407\u001b[0m, in \u001b[0;36mtrain_test\u001b[0;34m(x_test, x_train, y_test, y_train, input_size, output_size, num_tests, optimizer, num_epochs, hybrid, lr, dr, filename)\u001b[0m\n\u001b[1;32m    399\u001b[0m x_test_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(x_test, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    400\u001b[0m y_test_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y_test, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m    402\u001b[0m (\n\u001b[1;32m    403\u001b[0m     final_train_loss,\n\u001b[1;32m    404\u001b[0m     final_val_loss,\n\u001b[1;32m    405\u001b[0m     final_val_accuracy,\n\u001b[1;32m    406\u001b[0m     total_epochs,\n\u001b[0;32m--> 407\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactive_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_val_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhybrid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m test_accuracy, loss \u001b[38;5;241m=\u001b[39m evaluate_model(\n\u001b[1;32m    422\u001b[0m     model, x_test_tensor, y_test_tensor, criterion\n\u001b[1;32m    423\u001b[0m )\n\u001b[1;32m    425\u001b[0m fold_train_losses\u001b[38;5;241m.\u001b[39mappend(final_train_loss)\n",
      "Cell \u001b[0;32mIn[34], line 183\u001b[0m, in \u001b[0;36mtrainer\u001b[0;34m(model, criterion, optimizer, early_stopping, x_train_tensor, y_train_tensor, x_val_tensor, y_val_tensor, num_epochs, hybrid, graph)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m    181\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m--> 183\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_train_tensor)\n\u001b[1;32m    186\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[33], line 23\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)))\n\u001b[1;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[0;32m---> 23\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     25\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"SGD best\"\"\"   \n",
    "\n",
    "set_seed(123)\n",
    "\n",
    "optimizer = optim.SGD\n",
    "\n",
    "train_test(\n",
    "    x_test,\n",
    "    x_train,\n",
    "    y_test,\n",
    "    y_train,\n",
    "    input_size,\n",
    "    output_size,\n",
    "    30,\n",
    "    optimizer,\n",
    "    num_epochs,\n",
    "    False,\n",
    "    0.005,\n",
    "    0.05,\n",
    "    \"SGD_test_results_best\",\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"Adam best\"\"\"\n",
    "\n",
    "set_seed(123)\n",
    "\n",
    "optimizer = optim.Adam\n",
    "\n",
    "train_test(\n",
    "    x_test,\n",
    "    x_train,\n",
    "    y_test,\n",
    "    y_train,\n",
    "    input_size,\n",
    "    output_size,\n",
    "    30,\n",
    "    optimizer,\n",
    "    num_epochs,\n",
    "    False,\n",
    "    0.0005,\n",
    "    0.05,\n",
    "    \"Adam_test_results_best\",\n",
    ")\n",
    "\n",
    "\"\"\"Rprop best\"\"\"\n",
    "\n",
    "set_seed(123)\n",
    "\n",
    "optimizer = optim.Rprop\n",
    "\n",
    "train_test(\n",
    "    x_test,\n",
    "    x_train,\n",
    "    y_test,\n",
    "    y_train,\n",
    "    input_size,\n",
    "    output_size,\n",
    "    30,\n",
    "    optimizer,\n",
    "    num_epochs,\n",
    "    False,\n",
    "    0.005,\n",
    "    0.05,\n",
    "    \"RProp_test_results_best\",\n",
    ")\n",
    "\n",
    "\"\"\"Hybrid best\"\"\"\n",
    "\n",
    "set_seed(123)\n",
    "\n",
    "hybrid_optimizers = {\"sgd\": optim.SGD, \"adam\": optim.Adam, \"rprop\": optim.Rprop}\n",
    "\n",
    "train_test(\n",
    "    x_test,\n",
    "    x_train,\n",
    "    y_test,\n",
    "    y_train,\n",
    "    input_size,\n",
    "    output_size,\n",
    "    30,\n",
    "    hybrid_optimizers,\n",
    "    num_epochs,\n",
    "    True,\n",
    "    0.005,\n",
    "    0.05,\n",
    "    \"Hybrid_test_results_best\",\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"SGD worst\"\"\"   \n",
    "\n",
    "set_seed(123)\n",
    "\n",
    "optimizer = optim.SGD\n",
    "\n",
    "train_test(\n",
    "    x_test,\n",
    "    x_train,\n",
    "    y_test,\n",
    "    y_train,\n",
    "    input_size,\n",
    "    output_size,\n",
    "    30,\n",
    "    optimizer,\n",
    "    num_epochs,\n",
    "    False,\n",
    "    0.0001,\n",
    "    0.5,\n",
    "    \"SGD_test_results_worst\",\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"Adam worst\"\"\"\n",
    "\n",
    "set_seed(123)\n",
    "\n",
    "optimizer = optim.Adam\n",
    "\n",
    "train_test(\n",
    "    x_test,\n",
    "    x_train,\n",
    "    y_test,\n",
    "    y_train,\n",
    "    input_size,\n",
    "    output_size,\n",
    "    30,\n",
    "    optimizer,\n",
    "    num_epochs,\n",
    "    False,\n",
    "    0.0001,\n",
    "    0.5,\n",
    "    \"Adam_test_results_worst\",\n",
    ")\n",
    "\n",
    "\"\"\"Rprop worst\"\"\"\n",
    "\n",
    "set_seed(123)\n",
    "\n",
    "optimizer = optim.Rprop\n",
    "\n",
    "train_test(\n",
    "    x_test,\n",
    "    x_train,\n",
    "    y_test,\n",
    "    y_train,\n",
    "    input_size,\n",
    "    output_size,\n",
    "    30,\n",
    "    optimizer,\n",
    "    num_epochs,\n",
    "    False,\n",
    "    0.0001,\n",
    "    0.5,\n",
    "    \"RProp_test_results_worst\",\n",
    ")\n",
    "\n",
    "\"\"\"Hybrid worst\"\"\"\n",
    "\n",
    "set_seed(123)\n",
    "\n",
    "hybrid_optimizers = {\"sgd\": optim.SGD, \"adam\": optim.Adam, \"rprop\": optim.Rprop}\n",
    "\n",
    "train_test(\n",
    "    x_test,\n",
    "    x_train,\n",
    "    y_test,\n",
    "    y_train,\n",
    "    input_size,\n",
    "    output_size,\n",
    "    30,\n",
    "    hybrid_optimizers,\n",
    "    num_epochs,\n",
    "    True,\n",
    "    0.01,\n",
    "    0.0,\n",
    "    \"Hybrid_test_results_worst\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
