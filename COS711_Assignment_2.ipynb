{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"Configuration\"\"\"\n",
    "\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "print_graphs = config['print_graphs']\n",
    "features = config['features']\n",
    "target = config['target']\n",
    "num_epochs = config['num_epochs']\n",
    "learning_rate = config['learning_rate']\n",
    "dropout_rate = config['dropout_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Classes\"\"\"\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_rate):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.bn4 = nn.BatchNorm1d(32)\n",
    "        self.output_layer = nn.Linear(32, output_size)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Functions\"\"\"\n",
    "\n",
    "\n",
    "def cap_outliers_percentiles(df, feature, lower_percentile=5, upper_percentile=95):\n",
    "    lower_limit = np.percentile(df[feature], lower_percentile)\n",
    "    upper_limit = np.percentile(df[feature], upper_percentile)\n",
    "\n",
    "    df[feature] = np.where(df[feature] < lower_limit, lower_limit, df[feature])\n",
    "    df[feature] = np.where(df[feature] > upper_limit, upper_limit, df[feature])\n",
    "\n",
    "\n",
    "def plot_feature_distributions(data, data_imputed, features):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, feature in enumerate(features):\n",
    "        plt.subplot(4, 3, i + 1)\n",
    "        sns.histplot(\n",
    "            data[feature].dropna(),\n",
    "            kde=True,\n",
    "            label=\"Before Imputation\",\n",
    "            color=\"blue\",\n",
    "            bins=30,\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        sns.histplot(\n",
    "            data_imputed[feature].dropna(),\n",
    "            kde=True,\n",
    "            label=\"After Imputation\",\n",
    "            color=\"red\",\n",
    "            bins=30,\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        plt.title(f\"{feature}\")\n",
    "        plt.xlabel(\"\")\n",
    "        plt.ylabel(\"\")\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_box_plots_comparison(data, data_imputed, features):\n",
    "    plt.figure(figsize=(15, 12))\n",
    "\n",
    "    for i, feature in enumerate(features):\n",
    "        plt.subplot(4, 3, i + 1)\n",
    "\n",
    "        df_before_plot = data[[feature]].copy()\n",
    "        df_before_plot[\"Imputation Status\"] = \"Before Imputation\"\n",
    "\n",
    "        df_after_plot = data_imputed[[feature]].copy()\n",
    "        df_after_plot[\"Imputation Status\"] = \"After Imputation\"\n",
    "\n",
    "        df_plot = pd.concat([df_before_plot, df_after_plot], ignore_index=True)\n",
    "\n",
    "        sns.boxplot(\n",
    "            x=\"Imputation Status\",\n",
    "            y=feature,\n",
    "            data=df_plot,\n",
    "            hue=\"Imputation Status\",\n",
    "            palette=\"Set2\",\n",
    "            showfliers=True,\n",
    "        )\n",
    "\n",
    "        plt.title(f\"{feature}\")\n",
    "        plt.xlabel(\"\")\n",
    "        plt.ylabel(\"\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_correlation_heatmaps(data, data_imputed, features):\n",
    "    corr_before = data[features].corr()\n",
    "    corr_after = data_imputed[features].corr()\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(corr_before, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    plt.title(\"Correlation Before Imputation\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.heatmap(corr_after, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    plt.title(\"Correlation After Imputation\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def single_plot_feature_distributions(data, features):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, feature in enumerate(features):\n",
    "        plt.subplot(4, 3, i + 1)\n",
    "        sns.histplot(\n",
    "            data[feature].dropna(),\n",
    "            kde=True,\n",
    "            label=\"Normalized\",\n",
    "            color=\"blue\",\n",
    "            bins=30,\n",
    "            alpha=1,\n",
    "        )\n",
    "        plt.title(f\"{feature}\")\n",
    "        plt.xlabel(\"\")\n",
    "        plt.ylabel(\"\")\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Read Data\"\"\"\n",
    "\n",
    "data = pd.read_csv(\"almond_data.csv\")\n",
    "\n",
    "data = data.iloc[:, 1:]\n",
    "\n",
    "data.head()\n",
    "\n",
    "data.columns = data.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Impute Features\"\"\"\n",
    "\n",
    "impute_features = [\n",
    "    \"Length (major axis)\",\n",
    "    \"Width (minor axis)\",\n",
    "    \"Thickness (depth)\",\n",
    "    \"Area\",\n",
    "    \"Perimeter\",\n",
    "    \"Solidity\",\n",
    "    \"Compactness\",\n",
    "    \"Extent\",\n",
    "    \"Convex hull(convex area)\",\n",
    "]\n",
    "\n",
    "knn_imputer = KNNImputer(n_neighbors=53)  # n_neighbors = root of dataset size\n",
    "\n",
    "data_imputed = data.copy()\n",
    "\n",
    "types = data[target].unique()\n",
    "for almond_type in types:\n",
    "\n",
    "    type_data = data[data[target] == almond_type].copy()\n",
    "\n",
    "    type_features = type_data[impute_features]\n",
    "\n",
    "    imputed_values = knn_imputer.fit_transform(type_features)\n",
    "\n",
    "    type_data[impute_features] = imputed_values\n",
    "\n",
    "    data_imputed.update(type_data)\n",
    "\n",
    "data_imputed[\"Roundness\"] = (4 * data_imputed[\"Area\"]) / (\n",
    "    np.pi * data_imputed[\"Length (major axis)\"] ** 2\n",
    ")\n",
    "\n",
    "data_imputed[\"Aspect Ratio\"] = (\n",
    "    data_imputed[\"Length (major axis)\"] / data_imputed[\"Width (minor axis)\"]\n",
    ")\n",
    "\n",
    "data_imputed[\"Eccentricity\"] = np.sqrt(\n",
    "    1 - (data_imputed[\"Width (minor axis)\"] / data_imputed[\"Length (major axis)\"]) ** 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cap Outliers\"\"\"\n",
    "\n",
    "for feature in features:\n",
    "    cap_outliers_percentiles(data_imputed, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_graphs:\n",
    "    plot_feature_distributions(data, data_imputed, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_graphs:\n",
    "    plot_box_plots_comparison(data, data_imputed, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_graphs:\n",
    "    plot_correlation_heatmaps(data, data_imputed, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Normalize Data\"\"\"\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "normalized_data = data_imputed.copy()\n",
    "normalized_data[features] = scaler.fit_transform(data_imputed[features])\n",
    "\n",
    "# print(normalized_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_graphs:\n",
    "    single_plot_feature_distributions(normalized_data, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/10000], Training Loss: 0.2665, Validation Loss: 0.2177\n",
      "Epoch [200/10000], Training Loss: 0.1577, Validation Loss: 0.1428\n",
      "Epoch [300/10000], Training Loss: 0.1029, Validation Loss: 0.1234\n",
      "Epoch [400/10000], Training Loss: 0.0852, Validation Loss: 0.1193\n",
      "Epoch [500/10000], Training Loss: 0.0834, Validation Loss: 0.1241\n",
      "Early stopping triggered, Epoch [502/10000]\n",
      "Final Validation Accuracy: 0.9590\n"
     ]
    }
   ],
   "source": [
    "x = normalized_data[features].values\n",
    "y = normalized_data[target].values\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(x, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "input_size = len(features)\n",
    "output_size = len(label_encoder.classes_)\n",
    "\n",
    "model = MLP(input_size, output_size, dropout_rate)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=50, delta=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val_tensor)\n",
    "        val_loss = criterion(val_outputs, y_val_tensor).item()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    early_stopping(val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(f'Early stopping triggered, Epoch [{epoch + 1}/{num_epochs}]')\n",
    "        break\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_outputs = model(X_val_tensor)\n",
    "    _, predicted = torch.max(val_outputs.data, 1)\n",
    "    accuracy = (predicted == y_val_tensor).float().mean()\n",
    "    print(f'Final Validation Accuracy: {accuracy:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
