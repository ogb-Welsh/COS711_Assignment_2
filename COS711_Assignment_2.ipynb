{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"Configuration\"\"\"\n",
    "\n",
    "with open(\"config.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# random_seed = random.randint(0, 100000)\n",
    "random_seed = 1\n",
    "\n",
    "print_graphs = config[\"print_graphs\"]\n",
    "features = config[\"features\"]\n",
    "target = config[\"target\"]\n",
    "num_epochs = config[\"num_epochs\"]\n",
    "learning_rate = config[\"learning_rate\"]\n",
    "dropout_rate = config[\"dropout_rate\"]\n",
    "train_batch_size = config[\"train_batch_size\"]\n",
    "val_batch_size = config[\"val_batch_size\"]\n",
    "test_batch_size = config[\"test_batch_size\"]\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Classes\"\"\"\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_rate):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.ln1 = nn.LayerNorm(256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.ln2 = nn.LayerNorm(128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.ln3 = nn.LayerNorm(64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.ln4 = nn.LayerNorm(32)\n",
    "        self.output_layer = nn.Linear(32, output_size)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.ln1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.ln2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.ln3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.ln4(self.fc4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Functions\"\"\"\n",
    "\n",
    "\n",
    "def cap_outliers_percentiles(df, feature, lower_percentile=5, upper_percentile=95):\n",
    "    lower_limit = np.percentile(df[feature], lower_percentile)\n",
    "    upper_limit = np.percentile(df[feature], upper_percentile)\n",
    "\n",
    "    df[feature] = np.where(df[feature] < lower_limit, lower_limit, df[feature])\n",
    "    df[feature] = np.where(df[feature] > upper_limit, upper_limit, df[feature])\n",
    "\n",
    "\n",
    "def plot_feature_distributions(data, data_imputed, features):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, feature in enumerate(features):\n",
    "        plt.subplot(4, 3, i + 1)\n",
    "        sns.histplot(\n",
    "            data[feature].dropna(),\n",
    "            kde=True,\n",
    "            label=\"Before Imputation\",\n",
    "            color=\"blue\",\n",
    "            bins=30,\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        sns.histplot(\n",
    "            data_imputed[feature].dropna(),\n",
    "            kde=True,\n",
    "            label=\"After Imputation\",\n",
    "            color=\"red\",\n",
    "            bins=30,\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        plt.title(f\"{feature}\")\n",
    "        plt.xlabel(\"\")\n",
    "        plt.ylabel(\"\")\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_box_plots_comparison(data, data_imputed, features):\n",
    "    plt.figure(figsize=(15, 12))\n",
    "\n",
    "    for i, feature in enumerate(features):\n",
    "        plt.subplot(4, 3, i + 1)\n",
    "\n",
    "        df_before_plot = data[[feature]].copy()\n",
    "        df_before_plot[\"Imputation Status\"] = \"Before Imputation\"\n",
    "\n",
    "        df_after_plot = data_imputed[[feature]].copy()\n",
    "        df_after_plot[\"Imputation Status\"] = \"After Imputation\"\n",
    "\n",
    "        df_plot = pd.concat([df_before_plot, df_after_plot], ignore_index=True)\n",
    "\n",
    "        sns.boxplot(\n",
    "            x=\"Imputation Status\",\n",
    "            y=feature,\n",
    "            data=df_plot,\n",
    "            hue=\"Imputation Status\",\n",
    "            palette=\"Set2\",\n",
    "            showfliers=True,\n",
    "        )\n",
    "\n",
    "        plt.title(f\"{feature}\")\n",
    "        plt.xlabel(\"\")\n",
    "        plt.ylabel(\"\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_correlation_heatmaps(data, data_imputed, features):\n",
    "    corr_before = data[features].corr()\n",
    "    corr_after = data_imputed[features].corr()\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(corr_before, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    plt.title(\"Correlation Before Imputation\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.heatmap(corr_after, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    plt.title(\"Correlation After Imputation\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def single_plot_feature_distributions(data, features):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, feature in enumerate(features):\n",
    "        plt.subplot(4, 3, i + 1)\n",
    "        sns.histplot(\n",
    "            data[feature].dropna(),\n",
    "            kde=True,\n",
    "            label=\"Normalized\",\n",
    "            color=\"blue\",\n",
    "            bins=30,\n",
    "            alpha=1,\n",
    "        )\n",
    "        plt.title(f\"{feature}\")\n",
    "        plt.xlabel(\"\")\n",
    "        plt.ylabel(\"\")\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def trainer(\n",
    "    model, criterion, optimizer, early_stopping, x_train_tensor, y_train_tensor, x_val_tensor, y_val_tensor, num_epochs\n",
    "):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        outputs = model(x_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_train_loss = loss.item()\n",
    "\n",
    "        val_accuracy, val_loss = evaluate_model(model, x_val_tensor, y_val_tensor, criterion)\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch + 1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\"\n",
    "            )\n",
    "\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping triggered, Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "            \n",
    "            accuracy, loss = evaluate_model(model, x_val_tensor, y_val_tensor, criterion)\n",
    "\n",
    "            print(f'Final Model Validation | Accuracy: {accuracy} Loss: {loss}')\n",
    "            break\n",
    "\n",
    "\n",
    "def evaluate_model(model, x_tensor, y_tensor, criterion):\n",
    "    model.eval()\n",
    "    total_samples = len(y_tensor)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(x_tensor)\n",
    "        loss = criterion(outputs, y_tensor).item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        accuracy = (predicted == y_tensor).sum().item() / total_samples\n",
    "\n",
    "    return accuracy, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Read Data\"\"\"\n",
    "\n",
    "data = pd.read_csv(\"almond_data.csv\")\n",
    "\n",
    "data = data.iloc[:, 1:]\n",
    "\n",
    "data.head()\n",
    "\n",
    "data.columns = data.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Impute Features\"\"\"\n",
    "\n",
    "impute_features = [\n",
    "    \"Length (major axis)\",\n",
    "    \"Width (minor axis)\",\n",
    "    \"Thickness (depth)\",\n",
    "    \"Area\",\n",
    "    \"Perimeter\",\n",
    "    \"Solidity\",\n",
    "    \"Compactness\",\n",
    "    \"Extent\",\n",
    "    \"Convex hull(convex area)\",\n",
    "]\n",
    "\n",
    "knn_imputer = KNNImputer(n_neighbors=53)  # n_neighbors = root of dataset size\n",
    "\n",
    "data_imputed = data.copy()\n",
    "\n",
    "types = data[target].unique()\n",
    "for almond_type in types:\n",
    "\n",
    "    type_data = data[data[target] == almond_type].copy()\n",
    "\n",
    "    type_features = type_data[impute_features]\n",
    "\n",
    "    imputed_values = knn_imputer.fit_transform(type_features)\n",
    "\n",
    "    type_data[impute_features] = imputed_values\n",
    "\n",
    "    data_imputed.update(type_data)\n",
    "\n",
    "data_imputed[\"Roundness\"] = (4 * data_imputed[\"Area\"]) / (\n",
    "    np.pi * data_imputed[\"Length (major axis)\"] ** 2\n",
    ")\n",
    "\n",
    "data_imputed[\"Aspect Ratio\"] = (\n",
    "    data_imputed[\"Length (major axis)\"] / data_imputed[\"Width (minor axis)\"]\n",
    ")\n",
    "\n",
    "data_imputed[\"Eccentricity\"] = np.sqrt(\n",
    "    1 - (data_imputed[\"Width (minor axis)\"] / data_imputed[\"Length (major axis)\"]) ** 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cap Outliers\"\"\"\n",
    "\n",
    "for feature in features:\n",
    "    cap_outliers_percentiles(data_imputed, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_graphs:\n",
    "    plot_feature_distributions(data, data_imputed, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_graphs:\n",
    "    plot_box_plots_comparison(data, data_imputed, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_graphs:\n",
    "    plot_correlation_heatmaps(data, data_imputed, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Normalize Data\"\"\"\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "normalized_data = data_imputed.copy()\n",
    "normalized_data[features] = scaler.fit_transform(data_imputed[features])\n",
    "\n",
    "# print(normalized_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_graphs:\n",
    "    single_plot_feature_distributions(normalized_data, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/10000], Training Loss: 0.3341, Validation Loss: 0.2169, Validation Accuracy: 0.9286\n",
      "Epoch [200/10000], Training Loss: 0.2003, Validation Loss: 0.1173, Validation Accuracy: 0.9667\n",
      "Early stopping triggered, Epoch [297/10000]\n",
      "Final Model Validation | Accuracy: 0.9714285714285714 Loss: 0.10964187979698181\n"
     ]
    }
   ],
   "source": [
    "x = normalized_data[features].values\n",
    "y = normalized_data[target].values\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(\n",
    "    x, y_encoded, test_size=0.3, random_state=random_seed\n",
    ")\n",
    "x_val, x_test, y_val, y_test = train_test_split(\n",
    "    x_temp, y_temp, test_size=0.5, random_state=random_seed\n",
    ")\n",
    "\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "x_val_tensor = torch.tensor(x_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "input_size = len(features)\n",
    "output_size = len(label_encoder.classes_)\n",
    "\n",
    "model = MLP(input_size, output_size, dropout_rate)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=50, delta=0.001)\n",
    "\n",
    "trainer(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    early_stopping,\n",
    "    x_train_tensor,\n",
    "    y_train_tensor,\n",
    "    x_val_tensor,\n",
    "    y_val_tensor,\n",
    "    num_epochs,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
