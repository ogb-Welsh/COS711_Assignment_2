{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "import yaml\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"Configuration\"\"\"\n",
    "\n",
    "with open(\"config.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# random_seed = random.randint(0, 100000)\n",
    "random_seed = 123\n",
    "\n",
    "print_graphs = config[\"print_graphs\"]\n",
    "perform_grid_search = config[\"perform_grid_search\"]\n",
    "features = config[\"features\"]\n",
    "target = config[\"target\"]\n",
    "num_epochs = config[\"num_epochs\"]\n",
    "learning_rate = config[\"learning_rate\"]\n",
    "dropout_rate = config[\"dropout_rate\"]\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Classes\"\"\"\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_rate):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.ln1 = nn.LayerNorm(256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.ln2 = nn.LayerNorm(128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.ln3 = nn.LayerNorm(64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.ln4 = nn.LayerNorm(32)\n",
    "        self.output_layer = nn.Linear(32, output_size)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.ln1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.ln2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.ln3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.ln4(self.fc4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "\n",
    "class HybridOptimizer:\n",
    "    def __init__(self, optimizers):\n",
    "        self.optimizers = optimizers\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for opt in self.optimizers:\n",
    "            opt.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        for opt in self.optimizers:\n",
    "            opt.step()\n",
    "\n",
    "    def average_step(self):\n",
    "        grad_accum = {}\n",
    "        num_opts = len(self.optimizers)\n",
    "\n",
    "        for opt in self.optimizers:\n",
    "            for group in opt.param_groups:\n",
    "                for param in group[\"params\"]:\n",
    "                    if param.grad is None:\n",
    "                        continue\n",
    "                    if param in grad_accum:\n",
    "                        grad_accum[param] += param.grad.clone()\n",
    "                    else:\n",
    "                        grad_accum[param] = param.grad.clone()\n",
    "\n",
    "        for param in grad_accum:\n",
    "            grad_accum[param] /= num_opts\n",
    "            param.grad = grad_accum[param]\n",
    "\n",
    "        for opt in self.optimizers:\n",
    "            opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Functions\"\"\"\n",
    "\n",
    "\n",
    "def cap_outliers_percentiles(df, feature, lower_percentile=5, upper_percentile=95):\n",
    "    lower_limit = np.percentile(df[feature], lower_percentile)\n",
    "    upper_limit = np.percentile(df[feature], upper_percentile)\n",
    "\n",
    "    df[feature] = np.where(df[feature] < lower_limit, lower_limit, df[feature])\n",
    "    df[feature] = np.where(df[feature] > upper_limit, upper_limit, df[feature])\n",
    "\n",
    "\n",
    "def plot_feature_distributions(data, data_imputed, features):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, feature in enumerate(features):\n",
    "        plt.subplot(4, 3, i + 1)\n",
    "        sns.histplot(\n",
    "            data[feature].dropna(),\n",
    "            kde=True,\n",
    "            label=\"Before Imputation\",\n",
    "            color=\"blue\",\n",
    "            bins=30,\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        sns.histplot(\n",
    "            data_imputed[feature].dropna(),\n",
    "            kde=True,\n",
    "            label=\"After Imputation\",\n",
    "            color=\"red\",\n",
    "            bins=30,\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        plt.title(f\"{feature}\")\n",
    "        plt.xlabel(\"\")\n",
    "        plt.ylabel(\"\")\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_box_plots_comparison(data, data_imputed, features):\n",
    "    plt.figure(figsize=(15, 12))\n",
    "\n",
    "    for i, feature in enumerate(features):\n",
    "        plt.subplot(4, 3, i + 1)\n",
    "\n",
    "        df_before_plot = data[[feature]].copy()\n",
    "        df_before_plot[\"Imputation Status\"] = \"Before Imputation\"\n",
    "\n",
    "        df_after_plot = data_imputed[[feature]].copy()\n",
    "        df_after_plot[\"Imputation Status\"] = \"After Imputation\"\n",
    "\n",
    "        df_plot = pd.concat([df_before_plot, df_after_plot], ignore_index=True)\n",
    "\n",
    "        sns.boxplot(\n",
    "            x=\"Imputation Status\",\n",
    "            y=feature,\n",
    "            data=df_plot,\n",
    "            hue=\"Imputation Status\",\n",
    "            palette=\"Set2\",\n",
    "            showfliers=True,\n",
    "        )\n",
    "\n",
    "        plt.title(f\"{feature}\")\n",
    "        plt.xlabel(\"\")\n",
    "        plt.ylabel(\"\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_correlation_heatmaps(data, data_imputed, features):\n",
    "    corr_before = data[features].corr()\n",
    "    corr_after = data_imputed[features].corr()\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(corr_before, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    plt.title(\"Correlation Before Imputation\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.heatmap(corr_after, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    plt.title(\"Correlation After Imputation\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def single_plot_feature_distributions(data, features):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, feature in enumerate(features):\n",
    "        plt.subplot(4, 3, i + 1)\n",
    "        sns.histplot(\n",
    "            data[feature].dropna(),\n",
    "            kde=True,\n",
    "            label=\"Normalized\",\n",
    "            color=\"blue\",\n",
    "            bins=30,\n",
    "            alpha=1,\n",
    "        )\n",
    "        plt.title(f\"{feature}\")\n",
    "        plt.xlabel(\"\")\n",
    "        plt.ylabel(\"\")\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def trainer(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    early_stopping,\n",
    "    x_train_tensor,\n",
    "    y_train_tensor,\n",
    "    x_val_tensor,\n",
    "    y_val_tensor,\n",
    "    num_epochs,\n",
    "    hybrid,\n",
    "):\n",
    "    gradient_data = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        outputs = model(x_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        if hybrid and isinstance(optimizer, HybridOptimizer):\n",
    "            grad_info = {}\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    grad_info[name] = param.grad.clone().cpu().numpy()\n",
    "            gradient_data.append(grad_info)\n",
    "\n",
    "            optimizer.average_step()\n",
    "        else:\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = loss.item()\n",
    "\n",
    "        val_accuracy, val_loss = evaluate_model(\n",
    "            model, x_val_tensor, y_val_tensor, criterion\n",
    "        )\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch + 1}/{num_epochs}], Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\"\n",
    "            )\n",
    "\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping triggered, Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "\n",
    "            return train_loss, val_loss, val_accuracy, (epoch + 1), gradient_data\n",
    "\n",
    "\n",
    "def evaluate_model(model, x_tensor, y_tensor, criterion):\n",
    "    model.eval()\n",
    "    total_samples = len(y_tensor)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(x_tensor)\n",
    "        loss = criterion(outputs, y_tensor).item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        accuracy = (predicted == y_tensor).sum().item() / total_samples\n",
    "\n",
    "    return accuracy, loss\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "def grid_search(\n",
    "    optimizers,\n",
    "    learning_rates,\n",
    "    dropout_rates,\n",
    "    input_size,\n",
    "    output_size,\n",
    "    x,\n",
    "    y_encoded,\n",
    "    num_epochs,\n",
    "    k_folds,\n",
    "    hybrid,\n",
    "):\n",
    "    date_time_str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    for optimizer_name, opt_class in optimizers.items():\n",
    "        filename = f\"results/grid_search_optimizer_{optimizer_name}_{date_time_str}.csv\"\n",
    "        for lr in learning_rates:\n",
    "            for dr in dropout_rates:\n",
    "                kf = KFold(n_splits=k_folds, shuffle=True, random_state=123)\n",
    "                fold_train_losses = []\n",
    "                fold_val_losses = []\n",
    "                fold_val_accuracies = []\n",
    "                fold_total_epochs = []\n",
    "\n",
    "                for fold, (train_idx, val_idx) in enumerate(kf.split(x)):\n",
    "                    set_seed(fold)\n",
    "\n",
    "                    print(\n",
    "                        f\"Learning Rate {lr}, Dropout Rate {dr}, Fold {fold + 1}/{k_folds}\"\n",
    "                    )\n",
    "\n",
    "                    x_train, x_val = x[train_idx], x[val_idx]\n",
    "                    y_train, y_val = y_encoded[train_idx], y_encoded[val_idx]\n",
    "\n",
    "                    x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "                    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "                    x_val_tensor = torch.tensor(x_val, dtype=torch.float32)\n",
    "                    y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "                    model = MLP(input_size, output_size, dr)\n",
    "                    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "                    if hybrid:\n",
    "                        hybrid_optimizers = [\n",
    "                            optimizers[\"sgd\"](model.parameters(), lr=lr),\n",
    "                            optimizers[\"adam\"](model.parameters(), lr=lr),\n",
    "                            optimizers[\"rprop\"](model.parameters()),\n",
    "                        ]\n",
    "                        optimizer = HybridOptimizer(hybrid_optimizers)\n",
    "                    else:\n",
    "                        optimizer = opt_class(model.parameters(), lr=lr)\n",
    "\n",
    "                    early_stopping = EarlyStopping(patience=50, delta=0.001)\n",
    "\n",
    "                    (\n",
    "                        final_train_loss,\n",
    "                        final_val_loss,\n",
    "                        final_val_accuracy,\n",
    "                        total_epochs, \n",
    "                        gradient_data\n",
    "                    ) = trainer(\n",
    "                        model,\n",
    "                        criterion,\n",
    "                        optimizer,\n",
    "                        early_stopping,\n",
    "                        x_train_tensor,\n",
    "                        y_train_tensor,\n",
    "                        x_val_tensor,\n",
    "                        y_val_tensor,\n",
    "                        num_epochs,\n",
    "                    )\n",
    "\n",
    "                    fold_train_losses.append(final_train_loss)\n",
    "                    fold_val_losses.append(final_val_loss)\n",
    "                    fold_val_accuracies.append(final_val_accuracy)\n",
    "                    fold_total_epochs.append(total_epochs)\n",
    "\n",
    "                avg_train_loss = np.mean(fold_train_losses)\n",
    "                avg_val_loss = np.mean(fold_val_losses)\n",
    "                avg_val_accuracy = np.mean(fold_val_accuracies)\n",
    "                avg_total_epochs = np.mean(fold_total_epochs)\n",
    "                std_train_loss = np.std(fold_val_losses)\n",
    "                std_val_loss = np.std(fold_val_losses)\n",
    "                std_val_accuracy = np.std(fold_val_losses)\n",
    "                std_total_epochs = np.std(fold_total_epochs)\n",
    "\n",
    "                save_grid_search_results(\n",
    "                    lr,\n",
    "                    dr,\n",
    "                    avg_train_loss,\n",
    "                    avg_val_loss,\n",
    "                    avg_val_accuracy,\n",
    "                    avg_total_epochs,\n",
    "                    std_train_loss,\n",
    "                    std_val_loss,\n",
    "                    std_val_accuracy,\n",
    "                    std_total_epochs,\n",
    "                    filename,\n",
    "                )\n",
    "\n",
    "\n",
    "def save_grid_search_results(\n",
    "    lr,\n",
    "    dr,\n",
    "    avg_train_loss,\n",
    "    avg_val_loss,\n",
    "    avg_val_accuracy,\n",
    "    avg_total_epochs,\n",
    "    std_train_loss,\n",
    "    std_val_loss,\n",
    "    std_val_accuracy,\n",
    "    std_total_epochs,\n",
    "    filename,\n",
    "):\n",
    "\n",
    "    header = [\n",
    "        \"learning_rate\",\n",
    "        \"dropout_rate\",\n",
    "        \"avg_train_loss\",\n",
    "        \"avg_val_loss\",\n",
    "        \"avg_val_accuracy\",\n",
    "        \"avg_total_epochs\",\n",
    "        \"std_train_loss\",\n",
    "        \"std_val_loss\",\n",
    "        \"std_val_accuracy\",\n",
    "        \"std_total_epochs\",\n",
    "    ]\n",
    "\n",
    "    row = [\n",
    "        lr,\n",
    "        dr,\n",
    "        avg_train_loss,\n",
    "        avg_val_loss,\n",
    "        avg_val_accuracy,\n",
    "        avg_total_epochs,\n",
    "        std_train_loss,\n",
    "        std_val_loss,\n",
    "        std_val_accuracy,\n",
    "        std_total_epochs,\n",
    "    ]\n",
    "\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    with open(filename, mode=\"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        if not file_exists:\n",
    "            writer.writerow(header)\n",
    "        writer.writerow(row)\n",
    "\n",
    "    print(f\"Results saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Read Data\"\"\"\n",
    "\n",
    "data = pd.read_csv(\"almond_data.csv\")\n",
    "\n",
    "data = data.iloc[:, 1:]\n",
    "\n",
    "data.head()\n",
    "\n",
    "data.columns = data.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Impute Features\"\"\"\n",
    "\n",
    "impute_features = [\n",
    "    \"Length (major axis)\",\n",
    "    \"Width (minor axis)\",\n",
    "    \"Thickness (depth)\",\n",
    "    \"Area\",\n",
    "    \"Perimeter\",\n",
    "    \"Solidity\",\n",
    "    \"Compactness\",\n",
    "    \"Extent\",\n",
    "    \"Convex hull(convex area)\",\n",
    "]\n",
    "\n",
    "knn_imputer = KNNImputer(n_neighbors=53)  # n_neighbors = root of dataset size\n",
    "\n",
    "data_imputed = data.copy()\n",
    "\n",
    "types = data[target].unique()\n",
    "for almond_type in types:\n",
    "\n",
    "    type_data = data[data[target] == almond_type].copy()\n",
    "\n",
    "    type_features = type_data[impute_features]\n",
    "\n",
    "    imputed_values = knn_imputer.fit_transform(type_features)\n",
    "\n",
    "    type_data[impute_features] = imputed_values\n",
    "\n",
    "    data_imputed.update(type_data)\n",
    "\n",
    "data_imputed[\"Roundness\"] = (4 * data_imputed[\"Area\"]) / (\n",
    "    np.pi * data_imputed[\"Length (major axis)\"] ** 2\n",
    ")\n",
    "\n",
    "data_imputed[\"Aspect Ratio\"] = (\n",
    "    data_imputed[\"Length (major axis)\"] / data_imputed[\"Width (minor axis)\"]\n",
    ")\n",
    "\n",
    "data_imputed[\"Eccentricity\"] = np.sqrt(\n",
    "    1 - (data_imputed[\"Width (minor axis)\"] / data_imputed[\"Length (major axis)\"]) ** 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cap Outliers\"\"\"\n",
    "\n",
    "for feature in features:\n",
    "    cap_outliers_percentiles(data_imputed, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_graphs:\n",
    "    plot_feature_distributions(data, data_imputed, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_graphs:\n",
    "    plot_box_plots_comparison(data, data_imputed, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_graphs:\n",
    "    plot_correlation_heatmaps(data, data_imputed, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Normalize Data\"\"\"\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "normalized_data = data_imputed.copy()\n",
    "normalized_data[features] = scaler.fit_transform(data_imputed[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "if print_graphs:\n",
    "    single_plot_feature_distributions(normalized_data, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data setup\"\"\"\n",
    "\n",
    "x = normalized_data[features].values\n",
    "y = normalized_data[target].values\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "input_size = len(features)\n",
    "output_size = len(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate 0.001, Dropout Rate 0.1, Fold 1/5\n",
      "Epoch [100/100000], Training Loss: 0.1965, Validation Loss: 0.2197, Validation Accuracy: 0.9180\n",
      "Epoch [200/100000], Training Loss: 0.1213, Validation Loss: 0.1492, Validation Accuracy: 0.9376\n",
      "Early stopping triggered, Epoch [289/100000]\n",
      "Learning Rate 0.001, Dropout Rate 0.1, Fold 2/5\n",
      "Epoch [100/100000], Training Loss: 0.2151, Validation Loss: 0.1686, Validation Accuracy: 0.9483\n",
      "Epoch [200/100000], Training Loss: 0.1204, Validation Loss: 0.1264, Validation Accuracy: 0.9626\n",
      "Epoch [300/100000], Training Loss: 0.0855, Validation Loss: 0.1163, Validation Accuracy: 0.9643\n",
      "Early stopping triggered, Epoch [378/100000]\n",
      "Learning Rate 0.001, Dropout Rate 0.1, Fold 3/5\n",
      "Epoch [100/100000], Training Loss: 0.1996, Validation Loss: 0.2539, Validation Accuracy: 0.9127\n",
      "Epoch [200/100000], Training Loss: 0.1078, Validation Loss: 0.2227, Validation Accuracy: 0.9287\n",
      "Early stopping triggered, Epoch [284/100000]\n",
      "Learning Rate 0.001, Dropout Rate 0.1, Fold 4/5\n",
      "Epoch [100/100000], Training Loss: 0.2081, Validation Loss: 0.1916, Validation Accuracy: 0.9286\n",
      "Epoch [200/100000], Training Loss: 0.1203, Validation Loss: 0.1357, Validation Accuracy: 0.9446\n",
      "Epoch [300/100000], Training Loss: 0.0799, Validation Loss: 0.1116, Validation Accuracy: 0.9500\n",
      "Early stopping triggered, Epoch [391/100000]\n",
      "Learning Rate 0.001, Dropout Rate 0.1, Fold 5/5\n",
      "Epoch [100/100000], Training Loss: 0.2131, Validation Loss: 0.1765, Validation Accuracy: 0.9339\n",
      "Epoch [200/100000], Training Loss: 0.1297, Validation Loss: 0.1237, Validation Accuracy: 0.9625\n",
      "Epoch [300/100000], Training Loss: 0.0826, Validation Loss: 0.1089, Validation Accuracy: 0.9625\n",
      "Early stopping triggered, Epoch [389/100000]\n",
      "Results saved to results/grid_search_optimizer_adam_2024-09-22_01-42-09.csv\n",
      "Learning Rate 0.001, Dropout Rate 0.2, Fold 1/5\n",
      "Epoch [100/100000], Training Loss: 0.3128, Validation Loss: 0.2509, Validation Accuracy: 0.9144\n",
      "Epoch [200/100000], Training Loss: 0.2087, Validation Loss: 0.1769, Validation Accuracy: 0.9305\n",
      "Epoch [300/100000], Training Loss: 0.1301, Validation Loss: 0.1578, Validation Accuracy: 0.9412\n",
      "Epoch [400/100000], Training Loss: 0.1346, Validation Loss: 0.1505, Validation Accuracy: 0.9447\n",
      "Early stopping triggered, Epoch [408/100000]\n",
      "Learning Rate 0.001, Dropout Rate 0.2, Fold 2/5\n",
      "Epoch [100/100000], Training Loss: 0.3279, Validation Loss: 0.2215, Validation Accuracy: 0.9340\n",
      "Epoch [200/100000], Training Loss: 0.2039, Validation Loss: 0.1408, Validation Accuracy: 0.9572\n",
      "Early stopping triggered, Epoch [294/100000]\n",
      "Learning Rate 0.001, Dropout Rate 0.2, Fold 3/5\n",
      "Epoch [100/100000], Training Loss: 0.2955, Validation Loss: 0.2866, Validation Accuracy: 0.9055\n",
      "Epoch [200/100000], Training Loss: 0.1715, Validation Loss: 0.2455, Validation Accuracy: 0.9198\n",
      "Epoch [300/100000], Training Loss: 0.1530, Validation Loss: 0.2249, Validation Accuracy: 0.9287\n",
      "Early stopping triggered, Epoch [369/100000]\n",
      "Learning Rate 0.001, Dropout Rate 0.2, Fold 4/5\n",
      "Epoch [100/100000], Training Loss: 0.3183, Validation Loss: 0.2474, Validation Accuracy: 0.8964\n",
      "Epoch [200/100000], Training Loss: 0.2018, Validation Loss: 0.1595, Validation Accuracy: 0.9321\n",
      "Epoch [300/100000], Training Loss: 0.1471, Validation Loss: 0.1415, Validation Accuracy: 0.9393\n",
      "Early stopping triggered, Epoch [323/100000]\n",
      "Learning Rate 0.001, Dropout Rate 0.2, Fold 5/5\n",
      "Epoch [100/100000], Training Loss: 0.3127, Validation Loss: 0.2397, Validation Accuracy: 0.9143\n",
      "Epoch [200/100000], Training Loss: 0.1992, Validation Loss: 0.1517, Validation Accuracy: 0.9446\n",
      "Epoch [300/100000], Training Loss: 0.1530, Validation Loss: 0.1141, Validation Accuracy: 0.9625\n",
      "Epoch [400/100000], Training Loss: 0.1316, Validation Loss: 0.1004, Validation Accuracy: 0.9714\n",
      "Early stopping triggered, Epoch [425/100000]\n",
      "Results saved to results/grid_search_optimizer_adam_2024-09-22_01-42-09.csv\n",
      "Learning Rate 0.01, Dropout Rate 0.1, Fold 1/5\n",
      "Epoch [100/100000], Training Loss: 0.1255, Validation Loss: 0.2069, Validation Accuracy: 0.9234\n",
      "Epoch [200/100000], Training Loss: 0.0674, Validation Loss: 0.1574, Validation Accuracy: 0.9537\n",
      "Early stopping triggered, Epoch [253/100000]\n",
      "Learning Rate 0.01, Dropout Rate 0.1, Fold 2/5\n",
      "Epoch [100/100000], Training Loss: 0.1370, Validation Loss: 0.1274, Validation Accuracy: 0.9643\n",
      "Early stopping triggered, Epoch [139/100000]\n",
      "Learning Rate 0.01, Dropout Rate 0.1, Fold 3/5\n",
      "Epoch [100/100000], Training Loss: 0.1057, Validation Loss: 0.2360, Validation Accuracy: 0.9269\n",
      "Early stopping triggered, Epoch [171/100000]\n",
      "Learning Rate 0.01, Dropout Rate 0.1, Fold 4/5\n",
      "Epoch [100/100000], Training Loss: 0.1313, Validation Loss: 0.1595, Validation Accuracy: 0.9321\n",
      "Epoch [200/100000], Training Loss: 0.0710, Validation Loss: 0.1353, Validation Accuracy: 0.9393\n",
      "Early stopping triggered, Epoch [209/100000]\n",
      "Learning Rate 0.01, Dropout Rate 0.1, Fold 5/5\n",
      "Epoch [100/100000], Training Loss: 0.1153, Validation Loss: 0.1398, Validation Accuracy: 0.9536\n",
      "Epoch [200/100000], Training Loss: 0.0464, Validation Loss: 0.1276, Validation Accuracy: 0.9679\n",
      "Early stopping triggered, Epoch [256/100000]\n",
      "Results saved to results/grid_search_optimizer_adam_2024-09-22_01-42-09.csv\n",
      "Learning Rate 0.01, Dropout Rate 0.2, Fold 1/5\n",
      "Epoch [100/100000], Training Loss: 0.1547, Validation Loss: 0.2215, Validation Accuracy: 0.9055\n",
      "Epoch [200/100000], Training Loss: 0.0938, Validation Loss: 0.1563, Validation Accuracy: 0.9465\n",
      "Early stopping triggered, Epoch [208/100000]\n",
      "Learning Rate 0.01, Dropout Rate 0.2, Fold 2/5\n",
      "Epoch [100/100000], Training Loss: 0.1615, Validation Loss: 0.1480, Validation Accuracy: 0.9572\n",
      "Early stopping triggered, Epoch [159/100000]\n",
      "Learning Rate 0.01, Dropout Rate 0.2, Fold 3/5\n",
      "Epoch [100/100000], Training Loss: 0.1637, Validation Loss: 0.2690, Validation Accuracy: 0.9144\n",
      "Early stopping triggered, Epoch [136/100000]\n",
      "Learning Rate 0.01, Dropout Rate 0.2, Fold 4/5\n",
      "Epoch [100/100000], Training Loss: 0.1709, Validation Loss: 0.1484, Validation Accuracy: 0.9393\n",
      "Early stopping triggered, Epoch [159/100000]\n",
      "Learning Rate 0.01, Dropout Rate 0.2, Fold 5/5\n",
      "Epoch [100/100000], Training Loss: 0.1763, Validation Loss: 0.1527, Validation Accuracy: 0.9268\n",
      "Epoch [200/100000], Training Loss: 0.0956, Validation Loss: 0.1073, Validation Accuracy: 0.9625\n",
      "Early stopping triggered, Epoch [282/100000]\n",
      "Results saved to results/grid_search_optimizer_adam_2024-09-22_01-42-09.csv\n",
      "Learning Rate 0.1, Dropout Rate 0.1, Fold 1/5\n",
      "Epoch [100/100000], Training Loss: 0.3698, Validation Loss: 0.3130, Validation Accuracy: 0.8930\n",
      "Epoch [200/100000], Training Loss: 0.1669, Validation Loss: 0.1988, Validation Accuracy: 0.9251\n",
      "Early stopping triggered, Epoch [240/100000]\n",
      "Learning Rate 0.1, Dropout Rate 0.1, Fold 2/5\n",
      "Epoch [100/100000], Training Loss: 0.2383, Validation Loss: 0.2118, Validation Accuracy: 0.9180\n",
      "Epoch [200/100000], Training Loss: 0.1050, Validation Loss: 0.1980, Validation Accuracy: 0.9465\n",
      "Early stopping triggered, Epoch [241/100000]\n",
      "Learning Rate 0.1, Dropout Rate 0.1, Fold 3/5\n",
      "Epoch [100/100000], Training Loss: 0.3422, Validation Loss: 0.3038, Validation Accuracy: 0.8645\n",
      "Epoch [200/100000], Training Loss: 0.1434, Validation Loss: 0.2745, Validation Accuracy: 0.9109\n",
      "Epoch [300/100000], Training Loss: 0.0908, Validation Loss: 0.2563, Validation Accuracy: 0.9305\n",
      "Early stopping triggered, Epoch [361/100000]\n",
      "Learning Rate 0.1, Dropout Rate 0.1, Fold 4/5\n",
      "Epoch [100/100000], Training Loss: 0.2576, Validation Loss: 0.2161, Validation Accuracy: 0.9071\n",
      "Epoch [200/100000], Training Loss: 0.1447, Validation Loss: 0.1517, Validation Accuracy: 0.9464\n",
      "Epoch [300/100000], Training Loss: 0.1067, Validation Loss: 0.1226, Validation Accuracy: 0.9589\n",
      "Early stopping triggered, Epoch [320/100000]\n",
      "Learning Rate 0.1, Dropout Rate 0.1, Fold 5/5\n",
      "Epoch [100/100000], Training Loss: 0.2621, Validation Loss: 0.2554, Validation Accuracy: 0.8911\n",
      "Epoch [200/100000], Training Loss: 0.1141, Validation Loss: 0.1492, Validation Accuracy: 0.9482\n",
      "Early stopping triggered, Epoch [284/100000]\n",
      "Results saved to results/grid_search_optimizer_adam_2024-09-22_01-42-09.csv\n",
      "Learning Rate 0.1, Dropout Rate 0.2, Fold 1/5\n",
      "Epoch [100/100000], Training Loss: 0.4874, Validation Loss: 0.4248, Validation Accuracy: 0.8217\n",
      "Epoch [200/100000], Training Loss: 0.2555, Validation Loss: 0.2066, Validation Accuracy: 0.9251\n",
      "Epoch [300/100000], Training Loss: 0.1587, Validation Loss: 0.2023, Validation Accuracy: 0.9323\n",
      "Early stopping triggered, Epoch [320/100000]\n",
      "Learning Rate 0.1, Dropout Rate 0.2, Fold 2/5\n",
      "Epoch [100/100000], Training Loss: 0.5936, Validation Loss: 0.5418, Validation Accuracy: 0.6916\n",
      "Epoch [200/100000], Training Loss: 0.2724, Validation Loss: 0.2029, Validation Accuracy: 0.9251\n",
      "Epoch [300/100000], Training Loss: 0.2076, Validation Loss: 0.1566, Validation Accuracy: 0.9483\n",
      "Early stopping triggered, Epoch [387/100000]\n",
      "Learning Rate 0.1, Dropout Rate 0.2, Fold 3/5\n",
      "Epoch [100/100000], Training Loss: 0.6715, Validation Loss: 0.6836, Validation Accuracy: 0.5989\n",
      "Epoch [200/100000], Training Loss: 0.6081, Validation Loss: 0.7420, Validation Accuracy: 0.7986\n",
      "Early stopping triggered, Epoch [216/100000]\n",
      "Learning Rate 0.1, Dropout Rate 0.2, Fold 4/5\n",
      "Epoch [100/100000], Training Loss: 0.4900, Validation Loss: 0.4631, Validation Accuracy: 0.8000\n",
      "Epoch [200/100000], Training Loss: 0.3008, Validation Loss: 0.2978, Validation Accuracy: 0.8946\n",
      "Epoch [300/100000], Training Loss: 0.1964, Validation Loss: 0.1915, Validation Accuracy: 0.9357\n",
      "Early stopping triggered, Epoch [363/100000]\n",
      "Learning Rate 0.1, Dropout Rate 0.2, Fold 5/5\n",
      "Epoch [100/100000], Training Loss: 0.8072, Validation Loss: 0.7217, Validation Accuracy: 0.6196\n",
      "Epoch [200/100000], Training Loss: 0.6981, Validation Loss: 0.6044, Validation Accuracy: 0.7500\n",
      "Epoch [300/100000], Training Loss: 0.5079, Validation Loss: 0.4265, Validation Accuracy: 0.8804\n",
      "Epoch [400/100000], Training Loss: 0.5367, Validation Loss: 0.3960, Validation Accuracy: 0.8732\n",
      "Epoch [500/100000], Training Loss: 0.4499, Validation Loss: 0.3650, Validation Accuracy: 0.9036\n",
      "Epoch [600/100000], Training Loss: 0.5036, Validation Loss: 0.3406, Validation Accuracy: 0.9054\n",
      "Early stopping triggered, Epoch [623/100000]\n",
      "Results saved to results/grid_search_optimizer_adam_2024-09-22_01-42-09.csv\n",
      "Learning Rate 0.001, Dropout Rate 0.1, Fold 1/5\n",
      "Epoch [100/100000], Training Loss: 0.2574, Validation Loss: 0.2510, Validation Accuracy: 0.8984\n",
      "Epoch [200/100000], Training Loss: 0.2537, Validation Loss: 0.2372, Validation Accuracy: 0.9073\n",
      "Epoch [300/100000], Training Loss: 0.2296, Validation Loss: 0.2330, Validation Accuracy: 0.9037\n",
      "Early stopping triggered, Epoch [397/100000]\n",
      "Learning Rate 0.001, Dropout Rate 0.1, Fold 2/5\n",
      "Epoch [100/100000], Training Loss: 0.2758, Validation Loss: 0.1846, Validation Accuracy: 0.9340\n",
      "Epoch [200/100000], Training Loss: 0.2758, Validation Loss: 0.1791, Validation Accuracy: 0.9305\n",
      "Epoch [300/100000], Training Loss: 0.2610, Validation Loss: 0.1742, Validation Accuracy: 0.9358\n",
      "Early stopping triggered, Epoch [394/100000]\n",
      "Learning Rate 0.001, Dropout Rate 0.1, Fold 3/5\n",
      "Epoch [100/100000], Training Loss: 0.2422, Validation Loss: 0.2941, Validation Accuracy: 0.8930\n",
      "Epoch [200/100000], Training Loss: 0.2286, Validation Loss: 0.2793, Validation Accuracy: 0.8984\n",
      "Epoch [300/100000], Training Loss: 0.2317, Validation Loss: 0.2704, Validation Accuracy: 0.8984\n",
      "Early stopping triggered, Epoch [383/100000]\n",
      "Learning Rate 0.001, Dropout Rate 0.1, Fold 4/5\n",
      "Epoch [100/100000], Training Loss: 0.2608, Validation Loss: 0.2027, Validation Accuracy: 0.9250\n",
      "Epoch [200/100000], Training Loss: 0.2473, Validation Loss: 0.1863, Validation Accuracy: 0.9232\n",
      "Early stopping triggered, Epoch [251/100000]\n",
      "Learning Rate 0.001, Dropout Rate 0.1, Fold 5/5\n",
      "Epoch [100/100000], Training Loss: 0.2726, Validation Loss: 0.2069, Validation Accuracy: 0.9196\n",
      "Epoch [200/100000], Training Loss: 0.2465, Validation Loss: 0.1929, Validation Accuracy: 0.9250\n",
      "Epoch [300/100000], Training Loss: 0.2597, Validation Loss: 0.1879, Validation Accuracy: 0.9321\n",
      "Epoch [400/100000], Training Loss: 0.2441, Validation Loss: 0.1846, Validation Accuracy: 0.9321\n",
      "Early stopping triggered, Epoch [403/100000]\n",
      "Results saved to results/grid_search_optimizer_rprop_2024-09-22_01-42-09.csv\n",
      "Learning Rate 0.001, Dropout Rate 0.2, Fold 1/5\n",
      "Epoch [100/100000], Training Loss: 0.3582, Validation Loss: 0.2916, Validation Accuracy: 0.8824\n",
      "Epoch [200/100000], Training Loss: 0.3461, Validation Loss: 0.2760, Validation Accuracy: 0.8877\n",
      "Early stopping triggered, Epoch [259/100000]\n",
      "Learning Rate 0.001, Dropout Rate 0.2, Fold 2/5\n",
      "Epoch [100/100000], Training Loss: 0.4024, Validation Loss: 0.2693, Validation Accuracy: 0.8948\n",
      "Epoch [200/100000], Training Loss: 0.3633, Validation Loss: 0.2418, Validation Accuracy: 0.9091\n",
      "Epoch [300/100000], Training Loss: 0.3493, Validation Loss: 0.2340, Validation Accuracy: 0.9180\n",
      "Epoch [400/100000], Training Loss: 0.3367, Validation Loss: 0.2294, Validation Accuracy: 0.9216\n",
      "Epoch [500/100000], Training Loss: 0.3405, Validation Loss: 0.2252, Validation Accuracy: 0.9234\n",
      "Early stopping triggered, Epoch [524/100000]\n",
      "Learning Rate 0.001, Dropout Rate 0.2, Fold 3/5\n",
      "Epoch [100/100000], Training Loss: 0.3479, Validation Loss: 0.3148, Validation Accuracy: 0.8734\n",
      "Epoch [200/100000], Training Loss: 0.3158, Validation Loss: 0.3027, Validation Accuracy: 0.8913\n",
      "Epoch [300/100000], Training Loss: 0.3415, Validation Loss: 0.2985, Validation Accuracy: 0.8930\n",
      "Early stopping triggered, Epoch [376/100000]\n",
      "Learning Rate 0.001, Dropout Rate 0.2, Fold 4/5\n",
      "Epoch [100/100000], Training Loss: 0.3760, Validation Loss: 0.2845, Validation Accuracy: 0.8839\n",
      "Epoch [200/100000], Training Loss: 0.3565, Validation Loss: 0.2586, Validation Accuracy: 0.9054\n",
      "Epoch [300/100000], Training Loss: 0.3574, Validation Loss: 0.2475, Validation Accuracy: 0.9071\n",
      "Epoch [400/100000], Training Loss: 0.3565, Validation Loss: 0.2409, Validation Accuracy: 0.9071\n",
      "Early stopping triggered, Epoch [419/100000]\n",
      "Learning Rate 0.001, Dropout Rate 0.2, Fold 5/5\n",
      "Epoch [100/100000], Training Loss: 0.3629, Validation Loss: 0.2778, Validation Accuracy: 0.8839\n",
      "Epoch [200/100000], Training Loss: 0.3267, Validation Loss: 0.2495, Validation Accuracy: 0.9036\n",
      "Epoch [300/100000], Training Loss: 0.3550, Validation Loss: 0.2416, Validation Accuracy: 0.9071\n",
      "Epoch [400/100000], Training Loss: 0.3296, Validation Loss: 0.2386, Validation Accuracy: 0.9071\n",
      "Early stopping triggered, Epoch [470/100000]\n",
      "Results saved to results/grid_search_optimizer_rprop_2024-09-22_01-42-09.csv\n",
      "Learning Rate 0.01, Dropout Rate 0.1, Fold 1/5\n",
      "Epoch [100/100000], Training Loss: 0.2205, Validation Loss: 0.2293, Validation Accuracy: 0.9073\n",
      "Epoch [200/100000], Training Loss: 0.2063, Validation Loss: 0.2169, Validation Accuracy: 0.9144\n",
      "Early stopping triggered, Epoch [273/100000]\n",
      "Learning Rate 0.01, Dropout Rate 0.1, Fold 2/5\n",
      "Epoch [100/100000], Training Loss: 0.2167, Validation Loss: 0.1684, Validation Accuracy: 0.9430\n",
      "Epoch [200/100000], Training Loss: 0.2091, Validation Loss: 0.1603, Validation Accuracy: 0.9501\n",
      "Epoch [300/100000], Training Loss: 0.2119, Validation Loss: 0.1580, Validation Accuracy: 0.9537\n",
      "Epoch [400/100000], Training Loss: 0.1958, Validation Loss: 0.1574, Validation Accuracy: 0.9537\n",
      "Early stopping triggered, Epoch [400/100000]\n",
      "Learning Rate 0.01, Dropout Rate 0.1, Fold 3/5\n",
      "Epoch [100/100000], Training Loss: 0.2160, Validation Loss: 0.2720, Validation Accuracy: 0.9037\n",
      "Early stopping triggered, Epoch [122/100000]\n",
      "Learning Rate 0.01, Dropout Rate 0.1, Fold 4/5\n",
      "Epoch [100/100000], Training Loss: 0.2267, Validation Loss: 0.1949, Validation Accuracy: 0.9268\n",
      "Epoch [200/100000], Training Loss: 0.2183, Validation Loss: 0.1806, Validation Accuracy: 0.9232\n",
      "Early stopping triggered, Epoch [281/100000]\n",
      "Learning Rate 0.01, Dropout Rate 0.1, Fold 5/5\n",
      "Epoch [100/100000], Training Loss: 0.2251, Validation Loss: 0.1872, Validation Accuracy: 0.9268\n",
      "Epoch [200/100000], Training Loss: 0.1996, Validation Loss: 0.1767, Validation Accuracy: 0.9250\n",
      "Early stopping triggered, Epoch [258/100000]\n",
      "Results saved to results/grid_search_optimizer_rprop_2024-09-22_01-42-09.csv\n",
      "Learning Rate 0.01, Dropout Rate 0.2, Fold 1/5\n",
      "Epoch [100/100000], Training Loss: 0.3246, Validation Loss: 0.2642, Validation Accuracy: 0.9091\n",
      "Epoch [200/100000], Training Loss: 0.3213, Validation Loss: 0.2566, Validation Accuracy: 0.9091\n",
      "Epoch [300/100000], Training Loss: 0.3082, Validation Loss: 0.2509, Validation Accuracy: 0.9091\n",
      "Early stopping triggered, Epoch [343/100000]\n",
      "Learning Rate 0.01, Dropout Rate 0.2, Fold 2/5\n",
      "Epoch [100/100000], Training Loss: 0.3154, Validation Loss: 0.2157, Validation Accuracy: 0.9180\n",
      "Epoch [200/100000], Training Loss: 0.3181, Validation Loss: 0.2052, Validation Accuracy: 0.9216\n",
      "Early stopping triggered, Epoch [244/100000]\n",
      "Learning Rate 0.01, Dropout Rate 0.2, Fold 3/5\n",
      "Epoch [100/100000], Training Loss: 0.3092, Validation Loss: 0.2978, Validation Accuracy: 0.8913\n",
      "Epoch [200/100000], Training Loss: 0.2942, Validation Loss: 0.2880, Validation Accuracy: 0.8895\n",
      "Early stopping triggered, Epoch [208/100000]\n",
      "Learning Rate 0.01, Dropout Rate 0.2, Fold 4/5\n",
      "Epoch [100/100000], Training Loss: 0.3215, Validation Loss: 0.2399, Validation Accuracy: 0.9125\n",
      "Epoch [200/100000], Training Loss: 0.3124, Validation Loss: 0.2282, Validation Accuracy: 0.9196\n",
      "Epoch [300/100000], Training Loss: 0.3126, Validation Loss: 0.2212, Validation Accuracy: 0.9179\n",
      "Epoch [400/100000], Training Loss: 0.3288, Validation Loss: 0.2177, Validation Accuracy: 0.9179\n",
      "Early stopping triggered, Epoch [403/100000]\n",
      "Learning Rate 0.01, Dropout Rate 0.2, Fold 5/5\n",
      "Epoch [100/100000], Training Loss: 0.3142, Validation Loss: 0.2415, Validation Accuracy: 0.9036\n",
      "Epoch [200/100000], Training Loss: 0.2936, Validation Loss: 0.2260, Validation Accuracy: 0.9054\n",
      "Early stopping triggered, Epoch [243/100000]\n",
      "Results saved to results/grid_search_optimizer_rprop_2024-09-22_01-42-09.csv\n",
      "Learning Rate 0.1, Dropout Rate 0.1, Fold 1/5\n",
      "Epoch [100/100000], Training Loss: 0.2603, Validation Loss: 0.2628, Validation Accuracy: 0.8984\n",
      "Early stopping triggered, Epoch [198/100000]\n",
      "Learning Rate 0.1, Dropout Rate 0.1, Fold 2/5\n",
      "Epoch [100/100000], Training Loss: 0.2399, Validation Loss: 0.1703, Validation Accuracy: 0.9465\n",
      "Epoch [200/100000], Training Loss: 0.2007, Validation Loss: 0.1551, Validation Accuracy: 0.9501\n",
      "Epoch [300/100000], Training Loss: 0.1856, Validation Loss: 0.1488, Validation Accuracy: 0.9519\n",
      "Early stopping triggered, Epoch [390/100000]\n",
      "Learning Rate 0.1, Dropout Rate 0.1, Fold 3/5\n",
      "Epoch [100/100000], Training Loss: 0.2062, Validation Loss: 0.2499, Validation Accuracy: 0.9162\n",
      "Epoch [200/100000], Training Loss: 0.2078, Validation Loss: 0.2284, Validation Accuracy: 0.9180\n",
      "Early stopping triggered, Epoch [274/100000]\n",
      "Learning Rate 0.1, Dropout Rate 0.1, Fold 4/5\n",
      "Epoch [100/100000], Training Loss: 0.2216, Validation Loss: 0.1672, Validation Accuracy: 0.9232\n",
      "Epoch [200/100000], Training Loss: 0.1876, Validation Loss: 0.1485, Validation Accuracy: 0.9339\n",
      "Early stopping triggered, Epoch [269/100000]\n",
      "Learning Rate 0.1, Dropout Rate 0.1, Fold 5/5\n",
      "Epoch [100/100000], Training Loss: 0.2352, Validation Loss: 0.2111, Validation Accuracy: 0.9214\n",
      "Epoch [200/100000], Training Loss: 0.2028, Validation Loss: 0.1877, Validation Accuracy: 0.9179\n",
      "Early stopping triggered, Epoch [278/100000]\n",
      "Results saved to results/grid_search_optimizer_rprop_2024-09-22_01-42-09.csv\n",
      "Learning Rate 0.1, Dropout Rate 0.2, Fold 1/5\n",
      "Epoch [100/100000], Training Loss: 0.9739, Validation Loss: 0.2783, Validation Accuracy: 0.9037\n",
      "Epoch [200/100000], Training Loss: 0.3228, Validation Loss: 0.2598, Validation Accuracy: 0.9144\n",
      "Early stopping triggered, Epoch [258/100000]\n",
      "Learning Rate 0.1, Dropout Rate 0.2, Fold 2/5\n",
      "Epoch [100/100000], Training Loss: 0.3053, Validation Loss: 0.2266, Validation Accuracy: 0.9020\n",
      "Epoch [200/100000], Training Loss: 0.2896, Validation Loss: 0.2076, Validation Accuracy: 0.9127\n",
      "Epoch [300/100000], Training Loss: 0.2866, Validation Loss: 0.2005, Validation Accuracy: 0.9234\n",
      "Early stopping triggered, Epoch [377/100000]\n",
      "Learning Rate 0.1, Dropout Rate 0.2, Fold 3/5\n",
      "Epoch [100/100000], Training Loss: 0.3005, Validation Loss: 0.3059, Validation Accuracy: 0.8806\n",
      "Epoch [200/100000], Training Loss: 0.2710, Validation Loss: 0.2754, Validation Accuracy: 0.9020\n",
      "Epoch [300/100000], Training Loss: 0.2631, Validation Loss: 0.2689, Validation Accuracy: 0.9037\n",
      "Epoch [400/100000], Training Loss: 0.2741, Validation Loss: 0.2644, Validation Accuracy: 0.9037\n",
      "Early stopping triggered, Epoch [416/100000]\n",
      "Learning Rate 0.1, Dropout Rate 0.2, Fold 4/5\n",
      "Epoch [100/100000], Training Loss: 0.2840, Validation Loss: 0.2196, Validation Accuracy: 0.9107\n",
      "Epoch [200/100000], Training Loss: 0.2603, Validation Loss: 0.1966, Validation Accuracy: 0.9286\n",
      "Early stopping triggered, Epoch [260/100000]\n",
      "Learning Rate 0.1, Dropout Rate 0.2, Fold 5/5\n",
      "Epoch [100/100000], Training Loss: 0.3239, Validation Loss: 0.2737, Validation Accuracy: 0.8911\n",
      "Epoch [200/100000], Training Loss: 0.3115, Validation Loss: 0.2471, Validation Accuracy: 0.9000\n",
      "Epoch [300/100000], Training Loss: 0.3076, Validation Loss: 0.2397, Validation Accuracy: 0.9089\n",
      "Epoch [400/100000], Training Loss: 0.2991, Validation Loss: 0.2352, Validation Accuracy: 0.9054\n",
      "Early stopping triggered, Epoch [478/100000]\n",
      "Results saved to results/grid_search_optimizer_rprop_2024-09-22_01-42-09.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Grid search with k fold cross validation using seed 123\"\"\"\n",
    "\n",
    "if perform_grid_search:\n",
    "\n",
    "    # optimizers = {\"sgd\": optim.SGD, \"adam\": optim.Adam, \"rprop\": optim.Rprop}\n",
    "    optimizers = {\"adam\": optim.Adam, \"rprop\": optim.Rprop}\n",
    "    # learning_rates = [0.0001, 0.0003, 0.0005, 0.0008, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2]\n",
    "    # dropout_rates = [0.0, 0.03, 0.05, 0.08, 0.1, 0.2, 0.3, 0.4, 0.5, 0.7]\n",
    "    learning_rates = [\n",
    "        0.001,\n",
    "        0.01,\n",
    "        0.1,\n",
    "    ]\n",
    "    dropout_rates = [0.1, 0.2]\n",
    "\n",
    "    # os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"COS711_Assignment_2.ipynb\"\n",
    "    # wandb.init(project=\"COS 711_Assignment 2\")\n",
    "\n",
    "    set_seed(123)\n",
    "    # grid_search(\n",
    "    #     optimizers,\n",
    "    #     learning_rates,\n",
    "    #     dropout_rates,\n",
    "    #     input_size,\n",
    "    #     output_size,\n",
    "    #     x,\n",
    "    #     y_encoded,\n",
    "    #     num_epochs,\n",
    "    #     k_folds=5,\n",
    "    #     hybrid=False,\n",
    "    # )\n",
    "\n",
    "    grid_search(\n",
    "        optimizers,\n",
    "        learning_rates,\n",
    "        dropout_rates,\n",
    "        input_size,\n",
    "        output_size,\n",
    "        x,\n",
    "        y_encoded,\n",
    "        num_epochs,\n",
    "        k_folds=5,\n",
    "        hybrid = True\n",
    "    )\n",
    "\n",
    "    # wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_tensor size: torch.Size([2018, 12])\n",
      "y_train_tensor size: torch.Size([2018])\n",
      "x_val_tensor size: torch.Size([505, 12])\n",
      "y_val_tensor size: torch.Size([505])\n",
      "x_test_tensor size: torch.Size([280, 12])\n",
      "y_test_tensor size: torch.Size([280])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Train, validation and test set splitting\"\"\"\n",
    "\n",
    "set_seed(random_seed)\n",
    "\n",
    "x_test, x_temp, y_test, y_temp = train_test_split(\n",
    "    x, y_encoded, train_size=0.1, random_state=1\n",
    ")\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_temp, y_temp, train_size=0.8, random_state=random_seed\n",
    ")\n",
    "\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "x_val_tensor = torch.tensor(x_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "print(\"x_train_tensor size:\", x_train_tensor.size())\n",
    "print(\"y_train_tensor size:\", y_train_tensor.size())\n",
    "print(\"x_val_tensor size:\", x_val_tensor.size())\n",
    "print(\"y_val_tensor size:\", y_val_tensor.size())\n",
    "print(\"x_test_tensor size:\", x_test_tensor.size())\n",
    "print(\"y_test_tensor size:\", y_test_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100000], Training Loss: 0.3480, Validation Loss: 0.2546, Validation Accuracy: 0.9069\n",
      "Epoch [200/100000], Training Loss: 0.2162, Validation Loss: 0.1559, Validation Accuracy: 0.9505\n",
      "Epoch [300/100000], Training Loss: 0.1594, Validation Loss: 0.1426, Validation Accuracy: 0.9465\n",
      "Epoch [400/100000], Training Loss: 0.1261, Validation Loss: 0.1121, Validation Accuracy: 0.9644\n",
      "Early stopping triggered, Epoch [472/100000]\n",
      "Epoch [472/100000], Training Loss: 0.1006, Validation Loss: 0.1063, Validation Accuracy: 0.9663\n"
     ]
    }
   ],
   "source": [
    "model = MLP(input_size, output_size, dropout_rate)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=50, delta=0.001)\n",
    "\n",
    "final_train_loss, final_val_loss, final_val_accuracy, total_epochs = trainer(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    early_stopping,\n",
    "    x_train_tensor,\n",
    "    y_train_tensor,\n",
    "    x_val_tensor,\n",
    "    y_val_tensor,\n",
    "    num_epochs,\n",
    ")\n",
    "\n",
    "print(f\"Epoch [{total_epochs}/{num_epochs}], Training Loss: {final_train_loss:.4f}, Validation Loss: {final_val_loss:.4f}, Validation Accuracy: {final_val_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
